{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from numpy import cos, sin, arcsin, sqrt\n",
    "from math import radians\n",
    "import seaborn as sns\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# need for token (mapbox)\n",
    "px.set_mapbox_access_token(\"pk.eyJ1IjoiZmlsaXBrcmFzbmlxaSIsImEiOiJja2luOW9jdmgwa3J3MnpvNXhkNGJ6MWFtIn0.eevoM5byqvtc1nC0oXpuOw\")\n",
    "\n",
    "def haversine(row, lonlat):\n",
    "    lat1, lon1 = lonlat\n",
    "    lon2, lat2 = row['LNG'], row['LAT']\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * arcsin(sqrt(a)) \n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "def cell_from_coords(data, lonlat):\n",
    "    data['LAT'], data['LNG'] = pd.to_numeric(data['LAT_Y'],errors='coerce'), pd.to_numeric(data['LONG_X'],errors='coerce')\n",
    "    data['distance'] = data.apply(lambda row: haversine(row, lonlat), axis=1)\n",
    "    row = data.sort_values(by='distance').iloc[0:1]\n",
    "    data.drop(['distance', 'LAT', \"LNG\"], axis=1, inplace=True)\n",
    "    return row\n",
    "\n",
    "def normalize(data, column):\n",
    "    data.loc[:, column] = (data[column]-data[column].mean())/data[column].std()\n",
    "    return data\n",
    "\n",
    "def fix_coords(data):\n",
    "    data['LAT'], data['LNG'] = pd.to_numeric(data['LAT_Y'],errors='coerce'), pd.to_numeric(data['LONG_X'],errors='coerce')\n",
    "    return data\n",
    "\n",
    "def prepare_for_hexbin(dataframe, weekly = True, monthly = True):\n",
    "    if weekly:\n",
    "        data_groupped = dataframe.groupby('ECELL_ID').resample('W-Mon', on='Date').mean().reset_index().sort_values(by='Date')\n",
    "    elif monthly:\n",
    "        data_groupped = dataframe.groupby('ECELL_ID').resample('MS', on='Date').mean().reset_index().sort_values(by='Date') \n",
    "    else:\n",
    "        data_groupped = dataframe.groupby('ECELL_ID').resample('D', on='Date').mean().reset_index().sort_values(by='Date')\n",
    "    data_groupped = data_groupped.sort_values(by='Date')\n",
    "    data_groupped['DateString'] = data_groupped['Date'].map(lambda x: x.strftime('%j'))\n",
    "    data_groupped[\"DateString\"] = (data_groupped[\"DateString\"].astype(int)+1)//7\n",
    "    return data_groupped\n",
    "\n",
    "def prepare_for_timeseries(dataframe, weekly = True, monthly = True):\n",
    "    if weekly:\n",
    "        data_groupped = dataframe.groupby('COMUNE').resample('W-Mon', on='Date').sum().reset_index().sort_values(by='Date')\n",
    "    elif monthly:\n",
    "        data_groupped = dataframe.groupby('COMUNE').resample('MS', on='Date').sum().reset_index().sort_values(by='Date') \n",
    "    else:\n",
    "        data_groupped = dataframe.groupby('COMUNE').resample('D', on='Date').sum().reset_index().sort_values(by='Date')\n",
    "    data_groupped = data_groupped.sort_values(by='Date')\n",
    "    data_groupped['DateString'] = data_groupped['Date'].map(lambda x: x.strftime('%j'))\n",
    "    data_groupped[\"DateString\"] = (data_groupped[\"DateString\"].astype(int)+1)//7\n",
    "    return data_groupped\n",
    "\n",
    "def normalize_per_city(data, column, cities = [\"MILANO\", \"ROMA\", \"TORINO\", \"NAPOLI\", \"BERGAMO\"], city_column = \"COMUNE\"):\n",
    "    means = {city: data.where(lambda x: x[city_column] == city).dropna()[column].mean() for city in cities}#, \"ROMA\": daily_selected_data_groupped.where(lambda x: x.COMUNE ==\"ROMA\").dropna()[\"DL_VOL\"].mean()}\n",
    "    stds = {city: data.where(lambda x: x[city_column] == city).dropna()[column].std() for city in cities}\n",
    "    data = data.where(lambda x:x[column] > 0).dropna()# filtering before doing computations: some fields are wrongly 0\n",
    "    data[\"NORM_{}\".format(column)] = data.apply(lambda x: (x[column] - means[x[city_column]]) / stds[x[city_column]], axis=1)\n",
    "    return data\n",
    "\n",
    "def assign_constant_col(df, col, val):\n",
    "    df[col] = val\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df, col):\n",
    "    df = df.dropna(subset=[col])\n",
    "    df = df.loc[df[col] > 0]\n",
    "    return df[df[col] < np.percentile(df[col],95)]\n",
    "\n",
    "def normalize_cols_per_city(df, cols = [\"USERNUM_AVG\", \"Hin_Succ\", \"DL_VOL\"]):\n",
    "    for col in cols:\n",
    "        df = normalize_per_city(df, col)\n",
    "    return df\n",
    "\n",
    "def assign_period(dfs):\n",
    "    # assumes dfs to follow order of periods\n",
    "    for i, (df, period) in enumerate(zip(dfs, periods)):\n",
    "        dfs[i] = assign_constant_col(df, \"period\", period)\n",
    "    return dfs\n",
    "\n",
    "def normalize_per_city_dfs(dfs):\n",
    "    for i, df in enumerate(dfs):\n",
    "        dfs[i] = normalize_per_city(dfs[i], \"USERNUM_AVG\")\n",
    "        dfs[i] = normalize_per_city(dfs[i], \"Hin_Succ\")\n",
    "        dfs[i] = normalize_per_city(dfs[i], \"DL_VOL\")\n",
    "    return dfs\n",
    "\n",
    "def complete_dfs_ts(dfs):\n",
    "    dfs = assign_period(dfs)\n",
    "    dfs = normalize_per_city_dfs(dfs)\n",
    "    return dfs\n",
    "\n",
    "def normalize_df_by_num_cells(df, cols = [\"USERNUM_AVG\", \"Hin_Succ\", \"DL_VOL\"]):\n",
    "    num_cell = df.ECELL_ID.unique().shape[0]\n",
    "    for col in cols:\n",
    "        df[\"NORM_NUM_CELL_{}\".format(col)] = df[col] / num_cell\n",
    "    return df\n",
    "\n",
    "def normalize_df_traffic_by_factor(df, cols=[\"DL_VOL\", \"UL_VOL\"], factor=1000):\n",
    "    for col in cols:\n",
    "        df[col] = df[col] / factor\n",
    "    return df\n",
    "\n",
    "def avg_float(df, cols=[\"USERNUM_AVG\"]):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "def fill_with_areas(dates, dateRange, fig):\n",
    "    dates = dates.sort_values()\n",
    "    emptyDates = []\n",
    "    for i, currentDate in enumerate(dates):\n",
    "        if i < len(dates)-1:\n",
    "            nextDate = dates.iloc[i+1]\n",
    "            threshold = 1\n",
    "\n",
    "            # just check current and next: if they differ more than <threshold>, it's done\n",
    "            if dateRange == \"Day\":\n",
    "                diff = nextDate.dayofyear-currentDate.dayofyear\n",
    "                threshold = 2\n",
    "            elif dateRange == \"Week\":\n",
    "                diff = nextDate.week-currentDate.week\n",
    "            else:\n",
    "                diff = nextDate.month-currentDate.month\n",
    "\n",
    "            if diff < 0 or diff > threshold:\n",
    "                emptyDates.append((currentDate, nextDate))\n",
    "\n",
    "    for (currentDate, nextDate) in emptyDates:\n",
    "        fig.add_shape(type=\"rect\",\n",
    "            yref=\"paper\",\n",
    "            x0=currentDate, y0=0,\n",
    "            x1=nextDate, y1=1,\n",
    "            line=dict(\n",
    "                width=0,\n",
    "            ),\n",
    "            fillcolor='rgba(102, 173, 255, 0.4)',\n",
    "        )\n",
    "\n",
    "    # isolamento fiduciario per chi Ã¨ entrato in contatto\n",
    "    fig.add_shape(type=\"rect\",\n",
    "        yref=\"paper\",\n",
    "        x0='2020-02-21 09:30', y0=0,\n",
    "        x1='2020-03-08 09:30', y1=1,\n",
    "        line=dict(\n",
    "            width=0,\n",
    "        ),\n",
    "        fillcolor='rgba(255, 249, 133, 0.4)',\n",
    "    )\n",
    "\n",
    "    # lockdown 1\n",
    "    fig.add_shape(type=\"rect\",\n",
    "        yref=\"paper\",\n",
    "        x0='2020-03-08 09:30', y0=0,\n",
    "        x1='2020-04-14 09:30', y1=1,\n",
    "        line=dict(\n",
    "            width=0,\n",
    "        ),\n",
    "        fillcolor='rgba(235, 0, 0, 0.4)',\n",
    "    )\n",
    "\n",
    "    # apertura di alcuni servizi (es: negozi neonati)\n",
    "    fig.add_shape(type=\"rect\",\n",
    "        yref=\"paper\",\n",
    "        x0='2020-04-14 09:30', y0=0,\n",
    "        x1='2020-05-11 09:30', y1=1,\n",
    "        line=dict(\n",
    "            width=0,\n",
    "        ),\n",
    "        fillcolor='rgba(192, 95, 42, 0.4)',\n",
    "    )\n",
    "\n",
    "    # seconda ondata: lockdown\n",
    "    fig.add_shape(type=\"rect\",\n",
    "        yref=\"paper\",\n",
    "        x0='2020-11-06 09:30', y0=0,\n",
    "        x1='2020-12-13 09:30', y1=1,\n",
    "        line=dict(\n",
    "            width=0,\n",
    "        ),\n",
    "        fillcolor='rgba(255, 117, 102, 0.4)',\n",
    "    )\n",
    "\n",
    "    # zona gialla per molti\n",
    "    fig.add_shape(type=\"rect\",\n",
    "        yref=\"paper\",\n",
    "        x0='2020-12-13 09:30', y0=0,\n",
    "        x1='2020-12-23 09:30', y1=1,\n",
    "        line=dict(\n",
    "            width=0,\n",
    "        ),\n",
    "        fillcolor='rgba(255, 250, 102, 0.4)',\n",
    "    )\n",
    "\n",
    "    # zona rossa (ferie)\n",
    "    fig.add_shape(type=\"rect\",\n",
    "        yref=\"paper\",\n",
    "        x0='2020-12-23 09:30', y0=0,\n",
    "        x1='2020-12-31 09:30', y1=1,\n",
    "        line=dict(\n",
    "            width=0,\n",
    "        ),\n",
    "        fillcolor='rgba(255, 102, 102, 0.4)',\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "\n",
    "data_path = \"/Users/filipkrasniqi/Documents/Datasets.tmp/traffic-covid/\"\n",
    "cities = [\"Milano\", \"Roma\", \"Torino\", \"Napoli\", \"Bergamo\"]\n",
    "\n",
    "columns_all_data, required_columns_all_data = [\"DL_VOL\", \"Hin_Succ\", \"USERNUM_AVG\"], [\"LAT\", \"LNG\", \"City\", 'hour', 'Date', 'ECELL_ID', 'COMUNE']\n",
    "periods = [\"Dawn\", \"Morning\", \"Lunch\", \"Afternoon\", \"Evening\"]\n",
    "# columns to use for hexbin visualization\n",
    "columns, required_columns = [\"DL_VOL\", \"Hin_Succ\", \"USERNUM_AVG\"], [\"LAT\", \"LNG\", \"DateString\", \"City\"]\n",
    "\n",
    "SAVE_WITH_LESS_COLS = False\n",
    "SAVE_AGGREGATED = False\n",
    "FORCE_RELOAD_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data regarding Rome and Milan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reloading data in case they are already computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "savedFileNotExist = not os.path.exists(\"{}/saved\".format(data_path))\n",
    "RELOAD_DATA = FORCE_RELOAD_DATA | savedFileNotExist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data for Rome and Milan and adding coordinates as float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RELOAD_DATA:\n",
    "    data_milano = fix_coords(pd.read_pickle(\"{}LTE_1800_{}.pkl\".format(data_path, cities[0])))\n",
    "    data_milano_2 = fix_coords(pd.read_pickle(\"{}LTE_1800_{}_P2.pkl\".format(data_path, cities[0])))\n",
    "    data_milano_3 = fix_coords(pd.read_pickle(\"{}LTE_1800_{}_2wave.pkl\".format(data_path, cities[0])))\n",
    "    data_rome = fix_coords(pd.read_pickle(\"{}LTE_1800_{}.pkl\".format(data_path, cities[1])))\n",
    "    data_rome_2 = fix_coords(pd.read_pickle(\"{}LTE_1800_{}_2wave.pkl\".format(data_path, cities[1])))\n",
    "    data_turin = fix_coords(pd.read_pickle(\"{}LTE_1800_{}.pkl\".format(data_path, cities[2])))\n",
    "    data_turin_2 = fix_coords(pd.read_pickle(\"{}LTE_1800_{}_2wave.pkl\".format(data_path, cities[2])))\n",
    "    data_naples = fix_coords(pd.read_pickle(\"{}LTE_1800_{}.pkl\".format(data_path, cities[3])))\n",
    "    data_naples_2 = fix_coords(pd.read_pickle(\"{}LTE_1800_{}_2wave.pkl\".format(data_path, cities[3])))\n",
    "    data_bergamo = fix_coords(pd.read_pickle(\"{}LTE_1800_{}.pkl\".format(data_path, cities[4])))\n",
    "    data_bergamo_2 = fix_coords(pd.read_pickle(\"{}LTE_1800_{}_2wave.pkl\".format(data_path, cities[4])))\n",
    "\n",
    "    data_milano = pd.concat([data_milano, data_milano_2, data_milano_3])\n",
    "    data_rome = pd.concat([data_rome, data_rome_2])\n",
    "    data_turin = pd.concat([data_turin, data_turin_2])\n",
    "    data_naples = pd.concat([data_naples, data_naples_2])\n",
    "    data_bergamo = pd.concat([data_bergamo, data_bergamo_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "numCellsPerCity = {\"MILANO\": data_milano.ECELL_ID.unique().shape[0], \"ROMA\": data_rome.ECELL_ID.unique().shape[0],\\\n",
    "                   \"TORINO\": data_turin.ECELL_ID.unique().shape[0], \"NAPOLI\": data_naples.ECELL_ID.unique().shape[0], \\\n",
    "                  \"BERGAMO\": data_bergamo.ECELL_ID.unique().shape[0]}\n",
    "'''\n",
    "numCellsPerCity = {\"MILANO\": 1048, \"ROMA\": 1734,\"TORINO\": 574, \"NAPOLI\": 464, \"BERGAMO\": 88}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fasten stuff, I can reload precomputed data\n",
    "if not RELOAD_DATA:\n",
    "    ts_period_data_groupped_daily = pd.read_pickle(\"{}saved/TS_1800_daily.pkl\".format(data_path))\n",
    "    ts_period_data_groupped_weekly = pd.read_pickle(\"{}saved/TS_1800_weekly.pkl\".format(data_path))\n",
    "    ts_period_data_groupped_monthly = pd.read_pickle(\"{}saved/TS_1800_monthly.pkl\".format(data_path))\n",
    "    daily_selected_data_groupped = pd.read_pickle(\"{}saved/PERIOD_TS_1800_daily.pkl\".format(data_path))\n",
    "    weekly_selected_data_groupped = pd.read_pickle(\"{}saved/PERIOD_TS_1800_weekly.pkl\".format(data_path))\n",
    "    monthly_selected_data_groupped = pd.read_pickle(\"{}saved/PERIOD_TS_1800_monthly.pkl\".format(data_path))\n",
    "    hex_data_groupped_week = pd.read_pickle(\"{}saved/HEX_1800_weekly.pkl\".format(data_path))\n",
    "    hex_data_groupped_month = pd.read_pickle(\"{}saved/HEX_1800_monthly.pkl\".format(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing outliers for each column (*)\n",
    "Outliers = those outside the 95% confidence. This is done before merging data among the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# remove outliers from the three datasets regarding DL_VOL and Hin_Succ\n",
    "if RELOAD_DATA:\n",
    "    data_milano = remove_outliers(data_milano, \"DL_VOL\")\n",
    "    data_milano = remove_outliers(data_milano, \"Hin_Succ\")\n",
    "\n",
    "    data_rome = remove_outliers(data_rome, \"DL_VOL\")\n",
    "    data_rome = remove_outliers(data_rome, \"Hin_Succ\")\n",
    "\n",
    "    data_turin = remove_outliers(data_turin, \"DL_VOL\")\n",
    "    data_turin = remove_outliers(data_turin, \"Hin_Succ\")\n",
    "\n",
    "    data_naples = remove_outliers(data_naples, \"DL_VOL\")\n",
    "    data_naples = remove_outliers(data_naples, \"Hin_Succ\")\n",
    "\n",
    "    data_bergamo = remove_outliers(data_bergamo, \"DL_VOL\")\n",
    "    data_bergamo = remove_outliers(data_bergamo, \"Hin_Succ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28226335.04858216,\n",
       " 155424920087.19827,\n",
       " 26698649.653318807,\n",
       " 184624052012.15457,\n",
       " 23207487.32859474)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I suppose, from this, that rome and naples are to be divided by 1000 because of different provider\n",
    "data_milano[\"DL_VOL\"].mean(), data_rome[\"DL_VOL\"].mean(), data_turin[\"DL_VOL\"].mean(), data_naples[\"DL_VOL\"].mean(), data_bergamo[\"DL_VOL\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data for cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging them\n",
    "if RELOAD_DATA:\n",
    "    data_milano[\"City\"] = 0\n",
    "    data_rome[\"City\"] = 1\n",
    "    data_turin[\"City\"] = 2\n",
    "    data_naples[\"City\"] = 3\n",
    "    data_bergamo[\"City\"] = 4\n",
    "    data_rome, data_milano, data_turin, data_naples, data_bergamo = \\\n",
    "        avg_float(data_rome), avg_float(data_milano), avg_float(data_turin), avg_float(data_naples), avg_float(data_bergamo)\n",
    "    data_rome, data_naples = normalize_df_traffic_by_factor(data_rome), normalize_df_traffic_by_factor(data_naples)\n",
    "    data_rome, data_milano, data_turin, data_naples, data_bergamo = \\\n",
    "        normalize_df_by_num_cells(data_rome), normalize_df_by_num_cells(data_milano), \\\n",
    "        normalize_df_by_num_cells(data_turin), normalize_df_by_num_cells(data_naples), \\\n",
    "        normalize_df_by_num_cells(data_bergamo)\n",
    "    all_data = pd.concat([data_milano, data_rome, data_turin, data_naples, data_bergamo])\n",
    "    # adding hour to simplify computation later\n",
    "    all_data[\"hour\"] = [t.hour for t in pd.DatetimeIndex(all_data.Date)]\n",
    "    selected_all_data = all_data[columns_all_data+required_columns_all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_WITH_LESS_COLS:\n",
    "    selected_all_data.to_pickle(\"{}saved/1800_{}.pkl\".format(data_path, '_'.join(cities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing data to group them for period (*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RELOAD_DATA:\n",
    "    # getting data with relative period\n",
    "    evening_data = selected_all_data.where(lambda x: (x.hour >= 20) & (x.hour <= 23)).dropna()\n",
    "    afternoon_data = selected_all_data.where(lambda x: (x.hour >= 16) & (x.hour <= 19)).dropna()\n",
    "    lunch_data = selected_all_data.where(lambda x: (x.hour >= 12) & (x.hour <= 15)).dropna()\n",
    "    morning_data = selected_all_data.where(lambda x: (x.hour >= 8) & (x.hour <= 11)).dropna()\n",
    "    dawn_data = selected_all_data.where(lambda x: (x.hour >= 4) & (x.hour <= 7)).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping data for timeseries: data range (*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regardless the period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BELLA 1\n",
      "BELLA 2\n",
      "BELLA 3\n",
      "BELLA 4\n",
      "BELLA 5\n",
      "BELLA 6\n"
     ]
    }
   ],
   "source": [
    "if RELOAD_DATA:\n",
    "    data_groupped_day = prepare_for_timeseries(selected_all_data, weekly = False, monthly = False)\n",
    "    daily_selected_data_groupped = data_groupped_day[columns+required_columns + [\"Date\", \"COMUNE\"]].dropna()\n",
    "\n",
    "    daily_selected_data_groupped = normalize_per_city(daily_selected_data_groupped, \"USERNUM_AVG\")\n",
    "    daily_selected_data_groupped = normalize_per_city(daily_selected_data_groupped, \"Hin_Succ\")\n",
    "    daily_selected_data_groupped = normalize_per_city(daily_selected_data_groupped, \"DL_VOL\")\n",
    "\n",
    "    data_groupped_weekly = prepare_for_timeseries(selected_all_data, weekly = True, monthly = False)\n",
    "    weekly_selected_data_groupped = data_groupped_weekly[columns+required_columns + [\"Date\", \"COMUNE\"]].dropna()\n",
    "\n",
    "    weekly_selected_data_groupped = normalize_per_city(weekly_selected_data_groupped, \"USERNUM_AVG\")\n",
    "    weekly_selected_data_groupped = normalize_per_city(weekly_selected_data_groupped, \"Hin_Succ\")\n",
    "    weekly_selected_data_groupped = normalize_per_city(weekly_selected_data_groupped, \"DL_VOL\")\n",
    "\n",
    "    data_groupped_month = prepare_for_timeseries(selected_all_data, weekly = False, monthly = True)\n",
    "    monthly_selected_data_groupped = data_groupped_month[columns+required_columns + [\"Date\", \"COMUNE\"]].dropna()\n",
    "    print(\"BELLA 5\")\n",
    "\n",
    "    monthly_selected_data_groupped = normalize_per_city(monthly_selected_data_groupped, \"USERNUM_AVG\")\n",
    "    monthly_selected_data_groupped = normalize_per_city(monthly_selected_data_groupped, \"Hin_Succ\")\n",
    "    monthly_selected_data_groupped = normalize_per_city(monthly_selected_data_groupped, \"DL_VOL\")\n",
    "    print(\"BELLA 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting per period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RELOAD_DATA:\n",
    "    # resampling data for timeseries: per day\n",
    "    dfs_day = [prepare_for_timeseries(dawn_data, weekly = False, monthly = False), \\\n",
    "            prepare_for_timeseries(morning_data, weekly = False, monthly = False), \\\n",
    "            prepare_for_timeseries(lunch_data, weekly = False, monthly = False), \\\n",
    "            prepare_for_timeseries(afternoon_data, weekly = False, monthly=False), \\\n",
    "            prepare_for_timeseries(evening_data, weekly = False, monthly=False)]\n",
    "\n",
    "    # resampling data for timeseries: per week\n",
    "    dfs_week = [prepare_for_timeseries(dawn_data, weekly = True, monthly = False), \\\n",
    "            prepare_for_timeseries(morning_data, weekly = True, monthly = False), \\\n",
    "            prepare_for_timeseries(lunch_data, weekly = True, monthly = False), \\\n",
    "            prepare_for_timeseries(afternoon_data, weekly = True, monthly=False), \\\n",
    "            prepare_for_timeseries(evening_data, weekly = True, monthly=False)]\n",
    "\n",
    "    # resampling data for timeseries: per month\n",
    "    dfs_month = [prepare_for_timeseries(dawn_data, weekly = False, monthly = True), \\\n",
    "            prepare_for_timeseries(morning_data, weekly = False, monthly = True), \\\n",
    "            prepare_for_timeseries(lunch_data, weekly = False, monthly = True), \\\n",
    "            prepare_for_timeseries(afternoon_data, weekly = False, monthly=True), \\\n",
    "            prepare_for_timeseries(evening_data, weekly = False, monthly=True)]\n",
    "\n",
    "    # assigning period col and normalizing\n",
    "    dfs_day, dfs_week, dfs_month = complete_dfs_ts(dfs_day), complete_dfs_ts(dfs_week), complete_dfs_ts(dfs_month)\n",
    "    # creating final dfs for day, week or month\n",
    "    ts_period_data_groupped_daily, ts_period_data_groupped_weekly, ts_period_data_groupped_monthly = \\\n",
    "        pd.concat(dfs_day), pd.concat(dfs_week), pd.concat(dfs_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping data for hexbin: data range and cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regardless the period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RELOAD_DATA:\n",
    "    hex_data_groupped_week = prepare_for_hexbin(selected_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RELOAD_DATA:\n",
    "    hex_data_groupped_month = prepare_for_hexbin(selected_all_data, weekly = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RELOAD_DATA:\n",
    "    # columns to use for hexbin visualization\n",
    "    columns, required_columns = [\"DL_VOL\", \"Hin_Succ\", \"USERNUM_AVG\"], [\"LAT\", \"LNG\", \"DateString\", \"City\"]\n",
    "\n",
    "    # week: take groupped data and assign the column I need. Plus, fix the city\n",
    "    hex_week_selected_data_groupped = hex_data_groupped_week[columns+required_columns].dropna()\n",
    "    hex_week_selected_data_groupped[\"City\"] = hex_week_selected_data_groupped[\"City\"].astype(int)\n",
    "    hex_week_selected_data_groupped[\"City\"] = hex_week_selected_data_groupped[\"City\"].apply(lambda x: cities[x])\n",
    "\n",
    "    # month: do the same as week\n",
    "    hex_month_selected_data_groupped = hex_data_groupped_month[columns+required_columns].dropna()\n",
    "    hex_month_selected_data_groupped[\"City\"] = hex_month_selected_data_groupped[\"City\"].astype(int)\n",
    "    hex_month_selected_data_groupped[\"City\"] = hex_month_selected_data_groupped[\"City\"].apply(lambda x: cities[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_AGGREGATED:\n",
    "    ts_period_data_groupped_daily.to_pickle(\"{}saved/TS_1800_daily.pkl\".format(data_path))\n",
    "    ts_period_data_groupped_weekly.to_pickle(\"{}saved/TS_1800_weekly.pkl\".format(data_path))\n",
    "    ts_period_data_groupped_monthly.to_pickle(\"{}saved/TS_1800_monthly.pkl\".format(data_path))\n",
    "    daily_selected_data_groupped.to_pickle(\"{}saved/PERIOD_TS_1800_daily.pkl\".format(data_path))\n",
    "    weekly_selected_data_groupped.to_pickle(\"{}saved/PERIOD_TS_1800_weekly.pkl\".format(data_path))\n",
    "    monthly_selected_data_groupped.to_pickle(\"{}saved/PERIOD_TS_1800_monthly.pkl\".format(data_path))\n",
    "    hex_data_groupped_week.to_pickle(\"{}saved/HEX_1800_weekly.pkl\".format(data_path))\n",
    "    hex_data_groupped_month.to_pickle(\"{}saved/HEX_1800_monthly.pkl\".format(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare aggregated data for Milano and Rome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building visualization for both normalized and non-normalized case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRanges = [\"Week\", \"Month\", \"Day\"]\n",
    "norm_columns = [\"NORM_{}\".format(col) for col in columns]\n",
    "towns = daily_selected_data_groupped.COMUNE.unique()\n",
    "\n",
    "def build_app_timeseries(columns, dataRanges, towns):    \n",
    "    name = \"TIMESERIES_TOTAL_{}\".format(\"_\".join(columns))\n",
    "    app_timeseries = JupyterDash(name)\n",
    "\n",
    "    app_timeseries.layout = html.Div([\n",
    "    html.Label(\n",
    "        [\n",
    "            \"Data range\",\n",
    "            dcc.Dropdown(id=\"dateRange\",\n",
    "                         options=[{\"label\": x, \"value\": x} for x in dataRanges],\n",
    "                        value=dataRanges[0],\n",
    "                        clearable=False)\n",
    "        ]),\n",
    "    html.Label(\n",
    "        [\n",
    "            \"KPI\",\n",
    "            dcc.Dropdown(id=\"kpi\",\n",
    "                         options=[{\"label\": x, \"value\": x} for x in columns],\n",
    "                        value=columns[0],\n",
    "                        clearable=False)\n",
    "        ]),\n",
    "    html.Label(\n",
    "        [\n",
    "            \"Comune\",\n",
    "            dcc.Dropdown(id=\"towns\",\n",
    "                         options=[{\"label\": x, \"value\": x} for x in towns],\n",
    "                        value=towns[0],\n",
    "                        multi=True,\n",
    "                        clearable=True)\n",
    "        ],\n",
    "    ),\n",
    "    html.Label(\n",
    "        [\n",
    "            \"Normalize per #cell\",\n",
    "            dcc.Dropdown(id=\"normalizeNumCell\",\n",
    "                         options=[{\"label\": x, \"value\": x} for x in [\"Yes\", \"No\"]],\n",
    "                        value=\"No\",\n",
    "                        clearable=False)\n",
    "        ]),\n",
    "    html.Div(dcc.Graph(id=name))])\n",
    "\n",
    "    @app_timeseries.callback(\n",
    "    Output(name, \"figure\"), \n",
    "    [Input(\"dateRange\", \"value\"), Input(\"kpi\", \"value\"), Input(\"towns\", \"value\"), Input(\"normalizeNumCell\", \"value\")])\n",
    "    def display_map_period(dateRange, kpi, towns, normalizeNumCell):\n",
    "            \n",
    "        if isinstance(towns, str):\n",
    "            towns = [towns]\n",
    "\n",
    "        if dateRange == \"Day\":\n",
    "            selected_data_groupped = daily_selected_data_groupped\n",
    "        elif dateRange==\"Week\":\n",
    "            selected_data_groupped = weekly_selected_data_groupped\n",
    "        else:\n",
    "            selected_data_groupped = monthly_selected_data_groupped\n",
    "        \n",
    "        if towns is not None:\n",
    "            # selected_data_groupped = selected_data_groupped.where(lambda x:x.COMUNE==town).dropna()\n",
    "            query = ' | '.join([f'COMUNE==\"{t}\"' for t in towns])\n",
    "            selected_data_groupped = selected_data_groupped.query(query)\n",
    "            \n",
    "        if normalizeNumCell == \"Yes\":\n",
    "            selected_data_groupped[kpi] = selected_data_groupped.apply(lambda x: x[kpi] / numCellsPerCity[x.COMUNE], axis=1)\n",
    "\n",
    "        fig = px.line(selected_data_groupped, x='Date', y=kpi, color='COMUNE')\n",
    "        fig = fill_with_areas(selected_data_groupped['Date'], dateRange, fig)\n",
    "\n",
    "        return fig\n",
    "    \n",
    "    return app_timeseries\n",
    "\n",
    "app_timeseries_normalized, app_timeseries = build_app_timeseries(norm_columns, dataRanges, towns), build_app_timeseries(columns, dataRanges, towns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:16000/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ca1f6a60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app_timeseries.run_server(mode='inline', port=16000) # debug=True, use_reloader=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Comparing raw data between Rome and Milan gives interesting insights on how the mobile traffic is used in the two cities. We remind here that population in Rome is around 4M, while Milan has approximately 3M people (ratio Milan-Rome: 0.75).\n",
    "### Download\n",
    "When analysing data regarding downloads in the two cities, our data are not consistent at all. On average, the metropolitan city of Rome downloads 10000 more data than Milan; the difference is huge. This could be explained by different factors: for example, users may typically use **heavier applications**, for **longer time** (e.g.: people in Rome may use more 4G connection then WiFi even at home), and also the number of subscribers to our mobile operator may not be represented by the popilation ratio.\n",
    "### Avg. Number of users\n",
    "Analysing this KPI allows us to throw some more conclusions. As we can see from the plots, Milan has an average usage of 4M per week, while Rome is around 40-50M. This means that we have, on average, around 10 times the users of Milan. This partially explains the higher number of downloads in Rome; still, we can now conclude that people in Rome use much more the mobile network wrt Milan. Please have in mind that this information may be biased: indeed, if mobility is higher on one city wrt another, we would experience duplicates when counting users. For this reason, we compare also the last KPI.\n",
    "### Handover-IN (success)\n",
    "For our purpose, we'll consider the values prior to March, in particular those of February; by doing this, we are focusing on data that do not comprehend COVID-19 as a factor of variation of mobility. Indeed, something that is much more visible when analysing this feature is the drop around March, that can be seen in both situations. For a better comparison, we'll see the normalized data per city afterwards, so for now we'll just compare absolute values of Handover during February.\n",
    "\n",
    "Rome has a daily average of around 200M, while Milan lies around 14M. This means that we have much more mobility in Rome, and that the number of users as described above are much more biased in the Rome case than in Milan. Despite that, we can still draw the same conclusions about the usage of mobile traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:16002/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2713230a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app_timeseries_normalized.run_server(mode='inline', port=16002) # debug=True, use_reloader=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "To understand this, here we provide a brief description of what we did. Basically, for each considered KPI we normalized the data for each city separately. By doing this, we can no more compare the absolute values; the analysis will be much more qualitative and focused on the behaviour during the month to **compare the two cases**, to draw some conclusions again on the use of mobile networks and on how people reacted to COVID-19 restrictions, both in terms of mobility and of usage of the mobile networks.\n",
    "\n",
    "### Download\n",
    "Our expectations would be to see a similar normalized behaviour, maybe in a different way, but still, we expect that entering lockdown would decrease the usage of mobile networks. The daily time series gives pretty interesting insights. In particular, we can see how Milan followed the just described trend for most of the time, even though at the end (when reaching summer) the usage keeps dropping in the same way as during lockdown. Instead, Rome is actually behaving in a very different way. Indeed, we can see how in march we have a huge increase of the usage of the networks, and while ending the lockdown we it drops in a logarithmic fashion.\n",
    "\n",
    "We should try to motivate this two aspects:\n",
    "1) why Rome has experienced such an increases of usage in March -> people are less used to use Internet then in Milan, but use mobile connection insteade of WiFi???\n",
    "2) why Milan follows our ideal trend in until May, but then follows the lockdown trend even though the city should experience a more normal behaviour -> people in Milan use internet for other purposes, and they were aware of the fact that they should behave properly regardless from the Lockdown???\n",
    "\n",
    "### Avg. Number of users\n",
    "If compared with the download scenario, the trend makes complete sense. Indeed, we see again a slow but linear decrease of users for Milan, while Rome experiences first a huge increase when entering March, and then a decrease once lockdown finishes. So, nothing new if compared with download.\n",
    "\n",
    "### Handover-IN (success)\n",
    "We can see that when normalizing the data we see a pattern. Mobility seems to work similar in the two cases, when normalizing the KPI, even though Milan seems to be a bit more reluctant to mobility after the lockdown if compared to Rome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_app_timeseries_period(columns, dataRanges, towns, periods):\n",
    "    name = \"TIMESERIES_PERIOD_{}\".format(\"_\".join(columns))\n",
    "    app_period_timeseries = JupyterDash(name)\n",
    "\n",
    "    app_period_timeseries.layout = html.Div([\n",
    "    html.Label(\n",
    "        [\n",
    "            \"Data range\",\n",
    "            dcc.Dropdown(id=\"dateRange\",\n",
    "                         options=[{\"label\": x, \"value\": x} for x in dataRanges],\n",
    "                         value=dataRanges[0],\n",
    "                         clearable=False,),\n",
    "        ]\n",
    "    ),\n",
    "    html.Label(\n",
    "        [\n",
    "            \"Periods to show\",\n",
    "            dcc.Dropdown(id=\"periods\",\n",
    "                         options=[{\"label\": x, \"value\": x} for x in periods],\n",
    "                        value=periods[0],\n",
    "                        multi=True,\n",
    "                        clearable=False,),\n",
    "        ]\n",
    "    ),\n",
    "    html.Label(\n",
    "        [\n",
    "            \"KPI\",\n",
    "            dcc.Dropdown(id=\"kpi\",\n",
    "                         options=[{\"label\": x, \"value\": x} for x in columns],\n",
    "                        value=columns[0],\n",
    "                        clearable=False)\n",
    "        ]\n",
    "    ),\n",
    "    html.Label(\n",
    "        [\n",
    "            \"Comune\",\n",
    "            dcc.Dropdown(id=\"towns\",\n",
    "                         options=[{\"label\": x, \"value\": x} for x in towns],\n",
    "                        value=towns[0],\n",
    "                        multi=True,\n",
    "                        clearable=True)\n",
    "        ],\n",
    "    ),\n",
    "    html.Div(dcc.Graph(id=name))])\n",
    "    \n",
    "    @app_period_timeseries.callback(\n",
    "    Output(name, \"figure\"), \n",
    "    [Input(\"dateRange\", \"value\"), Input(\"periods\", \"value\"), Input(\"kpi\", \"value\"), Input(\"towns\", \"value\")])\n",
    "    def display_map_period(dateRange, periods, kpi, towns):\n",
    "        \n",
    "        if periods is None:\n",
    "            periods = [\"Dawn\"]\n",
    "            \n",
    "        if isinstance(periods, str):\n",
    "            periods = [periods]\n",
    "            \n",
    "        if isinstance(towns, str):\n",
    "            towns = [towns]\n",
    "            \n",
    "        if dateRange == \"Day\":\n",
    "            selected_data_groupped = ts_period_data_groupped_daily\n",
    "        elif dateRange==\"Week\":\n",
    "            selected_data_groupped = ts_period_data_groupped_weekly\n",
    "        else:\n",
    "            selected_data_groupped = ts_period_data_groupped_monthly\n",
    "        \n",
    "        if towns is not None:\n",
    "            query = ' | '.join([f'COMUNE==\"{t}\"' for t in towns])\n",
    "            selected_data_groupped= selected_data_groupped.query(query)\n",
    "            \n",
    "        # or condition on the filters\n",
    "        query = ' | '.join([f'period==\"{p}\"' for p in periods])\n",
    "        selected_data_groupped= selected_data_groupped.query(query)\n",
    "\n",
    "        fig = px.line(selected_data_groupped, x='Date', y=kpi, color='COMUNE', line_dash='period')\n",
    "        fig = fill_with_areas(selected_data_groupped['Date'], dateRange, fig)\n",
    "            \n",
    "        return fig\n",
    "    \n",
    "    return app_period_timeseries\n",
    "\n",
    "app_timeseries_period_normalized, app_timeseries_period = \\\n",
    "    build_app_timeseries_period(norm_columns, dataRanges, towns, ts_period_data_groupped_daily[\"period\"].unique()), \\\n",
    "    build_app_timeseries_period(columns, dataRanges, towns, ts_period_data_groupped_daily[\"period\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:16003/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2a97ad700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app_timeseries_period.run_server(mode='inline', port = 16003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:16004/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x25f0dfa00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app_timeseries_period_normalized.run_server(mode='inline', port = 16004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commenti\n",
    "- DL: evening in Roma meno dipendente da lockdown in confronto agli altri periodi\n",
    "- Hin ha comportamento simile in Lunch, Afternoon e Morning\n",
    "- 9 marzo: lockdown coincide con caduta di mobilitÃ  (Hin)\n",
    "- 9 Marzo: a Roma si nota un aumento di DL VOL. L'aumento Ã¨ notevole nell'orario lavorativo, segno che l'utilizzo del traffico internet mobile Ã¨ diverso nelle due cittÃ \n",
    "- 6 aprile: minimo Hin sia a Milano che a Roma https://www.reggionline.com/coronavirus-circa-2-500-aziende-riapriranno-deroga-dal-6-aprile-video/\n",
    "- 4 maggio: notevole aumento (inizio fase 2)\n",
    "- 1 giugno: diminuzione notevole, festa 2/06 e ponte\n",
    "- 02/07: Milano, evening: calo inspiegato di Hin\n",
    "- Estate: sia Milano che Roma registrano un maggiore aumento normalizzato di mobilitÃ  la sera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare aggregated data for different period of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo aggiungere torino -> sistemare la visualizzazione in mappa di torino\n",
    "# TODO dividere per 1000 i dati di volume di traffico: a milano e torino abbiamo kbit, roma bit -> X\n",
    "# TODO normalizzare numero di utenti per numero di celle del comune e volume di traffico per numero di celle -> X\n",
    "\n",
    "# TODO confrontare dati cablatura per comune per motivare meglio\n",
    "\n",
    "# idea: clusterizzare cittÃ  per \"qualitÃ  di cablatura\" per poi cercare di spiegare le differenze di traffico\n",
    "# TODO market share\n",
    "\n",
    "# TODO provare nuove features:\n",
    "# 2) - ratio at every time of dl_link -> how much link is used? this could normalize in case cells behave differently. Does this actually happen?\n",
    "# 1) - dl_link/#user -> low values should represent high mobility areas, because users connected but used few data compared to other\n",
    "\n",
    "# TODO add rolling average in visualizzazione\n",
    "\n",
    "# TODO timeseries covid\n",
    "\n",
    "# TODO parlare con Andrea per analizzare Roma (dritte su zone e altro)\n",
    "# TODO different KPIs -> Filip -> X\n",
    "# TODO per month instead of week -> Filip -> X\n",
    "\n",
    "# TODO represent in different period during days, and compare with dinner time (as a base for home) -> Filip -> X\n",
    "# TODO represent in different period of weeks, and compare with weekend -> Filip\n",
    "\n",
    "# TODO find info about density of population on specific places / areas -> Franci, (Filip)\n",
    "# TODO find area specification per area (e.g.: business, home, turistic, ...) -> Franci, (Filip)\n",
    "\n",
    "# -> brainstorming: attempt on 23/12\n",
    "\n",
    "# es: Garibaldi -> DoW, h8-h12, h14-h18: work\n",
    "# DoW + WE, h18-6: home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated data in map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:15000/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2646833d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app = JupyterDash(\"Map\")\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Label([\"KPIs\", dcc.Dropdown(\n",
    "        id=\"kpi\",\n",
    "        options=[{\"label\": x, \"value\": x} for x in columns],\n",
    "        value=columns[0],\n",
    "        clearable=False,\n",
    "                )]),\n",
    "    html.Label(\n",
    "        [\n",
    "            \"CittÃ \",\n",
    "            dcc.Dropdown(id=\"city\",\n",
    "                         options=[{\"label\": x, \"value\": x} for x in hex_week_selected_data_groupped.City.unique()],\n",
    "                        value=hex_week_selected_data_groupped[\"City\"].unique()[1],\n",
    "                        clearable=False,),\n",
    "        ]\n",
    "    ),\n",
    "    html.Label(\n",
    "        [\n",
    "            \"Data range\",\n",
    "            dcc.Dropdown(id=\"dateRange\",\n",
    "                         options=[{\"label\": x, \"value\": x} for x in [\"Week\", \"Month\"]],\n",
    "                        value=\"Week\",\n",
    "                        clearable=False,),\n",
    "        ]\n",
    "    ),\n",
    "    html.Div(dcc.Graph(id=\"map-chart\"))\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"map-chart\", \"figure\"), \n",
    "    [Input(\"kpi\", \"value\"), Input(\"city\", \"value\"), Input(\"dateRange\", \"value\")])\n",
    "\n",
    "def display_map(kpi, city, dateRange):\n",
    "    \n",
    "    if dateRange == \"Week\":\n",
    "        selected_data_groupped = hex_week_selected_data_groupped\n",
    "    else:\n",
    "        selected_data_groupped = hex_month_selected_data_groupped\n",
    "    \n",
    "    filtered_data_groupped = selected_data_groupped.where(lambda x:x.City==city).dropna()\n",
    "    n_hex = 20 if city == \"TORINO\" else 5 if city == \"BERGAMO\" else 30\n",
    "    fig = ff.create_hexbin_mapbox(\n",
    "        data_frame=filtered_data_groupped,\n",
    "        lat=\"LAT\", lon=\"LNG\", nx_hexagon=n_hex, animation_frame=\"DateString\", color=kpi,\n",
    "        color_continuous_scale=\"Inferno\", labels={\"color\": kpi, \"frame\": \"DateString\"}\n",
    "    )\n",
    "    fig.update_layout(margin=dict(b=0, t=0, l=0, r=0))\n",
    "    fig.layout.sliders[0].pad.t=20\n",
    "    fig.layout.updatemenus[0].pad.t=60\n",
    "    return fig\n",
    "\n",
    "app.run_server(mode='inline', port = 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO confrontare qualitÃ  della rete di dove stavano durante pre lockdown e post lockdown, o comunque dove veniva usato il traffico\n",
    "# altrimenti...\n",
    "# TODO l'idea sarebbe: per ogni istante di tempo, mediare, in base al numero utenti, il download e confrontare l'andamento con quello del traffico"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic-covid",
   "language": "python",
   "name": "traffic-covid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
