{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import gamma, poisson\n",
    "\n",
    "import epyestim\n",
    "import epyestim.covid19 as covid19\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import datetime\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE, r2_score\n",
    "from xgboost import XGBRegressor, DMatrix, train\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, TimeDistributed, RepeatVector\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import plotly.express as px\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "\n",
    "to_sum_KPIs = ['totale_casi_giornalieri', 'terapia_intensiva_giornalieri', 'terapia_intensiva', 'nuovi_positivi', 'tamponi_giornalieri']\n",
    "covidKPIsPrecompute = ['%pos']+to_sum_KPIs\n",
    "trafficKPIsPrecompute = ['Handover', 'Download vol.', 'Upload vol.', '#Users']\n",
    "\n",
    "# sums regions such as trento + bolzano\n",
    "def sumRegions(df, dateCol = 'Date', regionCol='Regione', cols = to_sum_KPIs, region1 = \"P.A. Bolzano\", region2 = \"P.A. Trento\", regionNew = \"Trentino-Alto Adige\"):\n",
    "    dfRegion1, dfRegion2 = df.loc[df[regionCol] == region1], df.loc[df[regionCol] == region2]\n",
    "    dfRegion1.set_index(dateCol, inplace=True)\n",
    "    dfRegion2.set_index(dateCol, inplace=True)\n",
    "    newVals = dfRegion1[to_sum_KPIs]+dfRegion2[to_sum_KPIs]\n",
    "    newVals.reset_index(inplace=True)\n",
    "    newVals['Regione'] = regionNew\n",
    "    df = df.loc[(df[regionCol] != region1) & (df[regionCol] != region2)]\n",
    "    return df.append(newVals)\n",
    "\n",
    "# adds italy as cumulative over days\n",
    "def addItalyData(df, cols):\n",
    "    dfTemp = df.resample('D', on='Date').sum().reset_index()\n",
    "    dfTemp['Regione']='Italia'\n",
    "    dfTemp = dfTemp[cols]\n",
    "    return pd.concat([df, dfTemp])\n",
    "\n",
    "def fill_with_areas(dateRange, fig, is_train):\n",
    "    if is_train:\n",
    "        color = 'rgba(255, 0, 0, 0.2)'\n",
    "    else:\n",
    "        color = 'rgba(0, 0, 255, 0.2)'\n",
    "    fig.add_shape(type=\"rect\",\n",
    "        yref=\"paper\",\n",
    "        x0=dateRange[0], y0=0,\n",
    "        x1=dateRange[-1], y1=1,\n",
    "        line=dict(\n",
    "            width=0,\n",
    "        ),\n",
    "        fillcolor=color,\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "data_path = \"/Users/filipkrasniqi/Documents/Datasets.tmp/traffic-covid/\"\n",
    "by_region_path = \"{}By_Region/\".format(data_path)\n",
    "saved = \"{}saved/\".format(data_path)\n",
    "traffic_daily = \"{}TS_1800_daily.pkl\".format(saved)\n",
    "region_traffic_daily = \"{}all.pkl\".format(saved)\n",
    "covid = \"{}covid/\".format(data_path)\n",
    "covid_daily = \"{}covid_2604.csv\".format(covid)\n",
    "\n",
    "capped_last_date = pd.to_datetime('2021-03-23')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle temperature data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle COVID data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompute_rt = True\n",
    "import_covid  = True\n",
    "path_covid = \"{}covid.csv\".format(saved)\n",
    "\n",
    "if import_covid:\n",
    "    df_covid = pd.read_csv(covid_daily, delimiter=';')\n",
    "    if \"Regione\" not in df_covid.columns:\n",
    "        df_covid.rename(columns={'denominazione_regione': 'Regione'}, inplace=True)\n",
    "        df_covid['tamponi_giornalieri'] = df_covid.groupby([\n",
    "                        'Regione'])['tamponi'].diff()\n",
    "        df_covid.loc[df_covid['tamponi_giornalieri'].isna() ,\n",
    "                               'tamponi_giornalieri'] = df_covid['tamponi']\n",
    "\n",
    "\n",
    "        df_covid['deceduti_giornalieri'] = df_covid.groupby([\n",
    "                            'Regione'])['deceduti'].diff()\n",
    "        df_covid.loc[df_covid['deceduti_giornalieri'].isna() ,\n",
    "                               'deceduti_giornalieri'] = df_covid['deceduti']\n",
    "\n",
    "        df_covid['terapia_intensiva_giornalieri'] = df_covid.groupby([\n",
    "                            'Regione'])['terapia_intensiva'].diff()\n",
    "        df_covid.loc[df_covid['terapia_intensiva_giornalieri'].isna() ,\n",
    "                               'terapia_intensiva_giornalieri'] = df_covid['terapia_intensiva']\n",
    "\n",
    "        df_covid['totale_casi_giornalieri'] = df_covid.groupby([\n",
    "                            'Regione'])['totale_casi'].diff()\n",
    "        df_covid.loc[df_covid['totale_casi_giornalieri'].isna() ,\n",
    "                               'totale_casi_giornalieri'] = df_covid['totale_casi']\n",
    "    covid_cols = ['Date', 'Regione', 'terapia_intensiva', 'nuovi_positivi', 'tamponi_giornalieri', 'totale_casi', 'deceduti', 'totale_casi_giornalieri', 'terapia_intensiva_giornalieri']\n",
    "    \n",
    "    try:\n",
    "        df_covid.data = pd.to_datetime(df_covid.data)\n",
    "        df_covid.rename(columns={'data': 'Date'}, inplace=True)\n",
    "    except:\n",
    "        pass # already correct name\n",
    "    df_covid = sumRegions(df_covid)\n",
    "    regions_covid = df_covid['Regione'].unique()\n",
    "    #df_covid = df_covid[df_covid['Regione'].isin(regions)].dropna()\n",
    "    df_covid.to_csv(path_covid)\n",
    "else:\n",
    "    try:\n",
    "        del df_covid\n",
    "    except:\n",
    "        print(\"No df covid\")\n",
    "        \n",
    "regions = df_covid.Regione.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Abruzzo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Basilicata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Calabria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Campania\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Emilia-Romagna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Friuli Venezia Giulia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Lazio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Liguria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Lombardia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Marche\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Molise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Piemonte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Puglia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Sardegna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Sicilia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Toscana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Umbria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Valle d'Aosta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Veneto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGIONE: Trentino-Alto Adige\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ef5bc409077c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
      "<ipython-input-64-ef5bc409077c>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  current_df['DateIndex'] = current_df.loc[:, 'Date']\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "path_covid_predictions=\"{}predictions/covid.pkl\".format(saved)\n",
    "if recompute_rt:\n",
    "    for r in regions:\n",
    "        print(\"REGIONE: {}\".format(r))\n",
    "        current_df = df_covid.loc[df_covid['Regione'] == r]\n",
    "        current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
    "        current_df['DateIndex'] = current_df.loc[:, 'Date']\n",
    "        current_df.set_index('DateIndex', inplace=True)\n",
    "        #current_df = current_df.loc[current_df['nuovi_positivi'] > 0]\n",
    "        current_df = current_df.loc[pd.to_datetime('2020/03/01'):]\n",
    "        idxs = (current_df['nuovi_positivi'] < 0)# | (current_df.isna()) | (current_df['nuovi_positivi'] == np.inf) | (current_df['nuovi_positivi'] == -np.inf)\n",
    "        if idxs.sum() > 0:\n",
    "            current_df.loc[idxs, 'nuovi_positivi'] = np.nan\n",
    "        current_df.fillna(method='ffill', inplace=True)\n",
    "        current_df.dropna(subset=['nuovi_positivi'], inplace=True)\n",
    "        #current_df[current_df.loc[:, 'nuovi_positivi']]\n",
    "        #current_df.dropna(subset=['nuovi_positivi'], inplace=True)\n",
    "        #\n",
    "        current_df = current_df.drop_duplicates(keep='first')\n",
    "        #print(current_df['nuovi_positivi'].shape, current_df['nuovi_positivi'].apply(lambda x: x < 0).sum())\n",
    "        #current_df.dropna(subset=['totale_casi_giornalieri'], inplace=True)\n",
    "        #print(current_df['totale_casi_giornalieri'].isna().sum())\n",
    "        #print(current_df['totale_casi_giornalieri'].sum())\n",
    "        r_t_series = covid19.r_covid(current_df['nuovi_positivi'])\n",
    "        current_df = pd.merge(current_df, r_t_series, left_index=True, right_index=True)\n",
    "        dfs.append(current_df)\n",
    "    df_covid_predictions = pd.concat(dfs)\n",
    "    del dfs\n",
    "    df_covid_predictions.set_index(['Date', 'Regione'], inplace=True)\n",
    "    df_covid_predictions['%pos'] = (df_covid_predictions['nuovi_positivi']/df_covid_predictions['tamponi_giornalieri'])\n",
    "    df_covid_predictions.to_pickle(path_covid_predictions)\n",
    "else:\n",
    "    df_covid_predictions = pd.read_pickle(path_covid_predictions)\n",
    "\n",
    "df_unseen_covid = df_covid_predictions.loc[df_covid_predictions.index.get_level_values('Date')>=capped_last_date]\n",
    "#df_covid_predictions = df_covid_predictions.loc[df_covid_predictions.index.get_level_values('Date')<capped_last_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid_predictions.loc[df_covid_predictions.index.get_level_values('Regione')=='Lombardia']['R_mean'].to_csv(\"{}predictions/covid_lombardia.csv\".format(saved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle traffic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_traffic = True\n",
    "recompute_kalman = False\n",
    "recompute_MA = True\n",
    "path_traffic = \"{}traffic.csv\".format(saved)\n",
    "start_date_unavailable, end_date_unavailable = pd.to_datetime('2020-08-01'), pd.to_datetime('2020-10-01')\n",
    "dates_unavailable = pd.date_range(start_date_unavailable, end_date_unavailable)\n",
    "if import_traffic:\n",
    "    df_traffic_daily = pd.read_pickle(region_traffic_daily)\n",
    "    df_traffic_daily.loc[df_traffic_daily['Regione'] == \"Emilia Romagna\", \"Regione\"] = \"Emilia-Romagna\"\n",
    "    df_traffic_predictions = df_traffic_daily.loc[df_traffic_daily['Regione'].isin(regions)]\n",
    "    df_traffic_predictions = df_traffic_predictions.groupby('Regione').resample('D', on='Date').sum().reset_index()\n",
    "    df_traffic_predictions = df_traffic_predictions.replace({'0':np.nan, 0:np.nan})\n",
    "    df_traffic_predictions = df_traffic_predictions.loc[(df_traffic_predictions['Date']<start_date_unavailable)|(df_traffic_predictions['Date']>=end_date_unavailable)]\n",
    "    df_traffic_predictions = df_traffic_predictions.fillna(method='ffill')\n",
    "    df_traffic_predictions['Date'] = pd.to_datetime(df_traffic_predictions['Date']).dt.date\n",
    "    df_traffic_predictions.set_index(['Date', 'Regione'], inplace=True)\n",
    "    df_traffic_predictions.to_csv(path_traffic)\n",
    "else:\n",
    "    df_traffic_predictions = pd.read_csv(path_traffic)\n",
    "    df_traffic_predictions['Date'] = pd.to_datetime(df_traffic_predictions['Date']).dt.date\n",
    "    df_traffic_predictions.set_index(['Date', 'Regione'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>RRC_S_Succ</th>\n",
       "      <th>RRC_S_Att</th>\n",
       "      <th>RRC_S_SR</th>\n",
       "      <th>RRC_RE_Succ</th>\n",
       "      <th>RRC_RE_Att</th>\n",
       "      <th>IntraF_Hout_Succ</th>\n",
       "      <th>IntraF_Hout_Att</th>\n",
       "      <th>InterF_Hout_Succ</th>\n",
       "      <th>InterF_Hout_Att</th>\n",
       "      <th>Handover</th>\n",
       "      <th>Hin_Att</th>\n",
       "      <th>InterR_HO_OUT_E2W_Succ</th>\n",
       "      <th>InterR_HO_OUT_E2W_Att</th>\n",
       "      <th>InterR_HO_OUT_E2G_Succ</th>\n",
       "      <th>InterR_HO_OUT_E2G_Att</th>\n",
       "      <th>Download vol.</th>\n",
       "      <th>Upload vol.</th>\n",
       "      <th>#Users</th>\n",
       "      <th>ERAB_S_Succ</th>\n",
       "      <th>ERAB_S_Att</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Regione</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <th>Abruzzo</th>\n",
       "      <td>722914109.0</td>\n",
       "      <td>723559625.0</td>\n",
       "      <td>2399.867376</td>\n",
       "      <td>2109077.0</td>\n",
       "      <td>2755617.0</td>\n",
       "      <td>69768501.0</td>\n",
       "      <td>69410898.0</td>\n",
       "      <td>29058705.0</td>\n",
       "      <td>29152448.0</td>\n",
       "      <td>87240956.0</td>\n",
       "      <td>88551230.0</td>\n",
       "      <td>106931.0</td>\n",
       "      <td>109647.0</td>\n",
       "      <td>39290.0</td>\n",
       "      <td>41672.0</td>\n",
       "      <td>2.892451e+15</td>\n",
       "      <td>2.471538e+14</td>\n",
       "      <td>3.184160e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <th>Abruzzo</th>\n",
       "      <td>718452155.0</td>\n",
       "      <td>718983916.0</td>\n",
       "      <td>2399.873855</td>\n",
       "      <td>2131711.0</td>\n",
       "      <td>2792486.0</td>\n",
       "      <td>84230949.0</td>\n",
       "      <td>83904966.0</td>\n",
       "      <td>28343453.0</td>\n",
       "      <td>28429081.0</td>\n",
       "      <td>103161979.0</td>\n",
       "      <td>104312783.0</td>\n",
       "      <td>166524.0</td>\n",
       "      <td>171219.0</td>\n",
       "      <td>51450.0</td>\n",
       "      <td>54111.0</td>\n",
       "      <td>2.675922e+15</td>\n",
       "      <td>2.237421e+14</td>\n",
       "      <td>3.220258e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <th>Abruzzo</th>\n",
       "      <td>732998734.0</td>\n",
       "      <td>733541088.0</td>\n",
       "      <td>2399.825795</td>\n",
       "      <td>2155915.0</td>\n",
       "      <td>2846211.0</td>\n",
       "      <td>86249305.0</td>\n",
       "      <td>85912857.0</td>\n",
       "      <td>29106383.0</td>\n",
       "      <td>29189094.0</td>\n",
       "      <td>105738844.0</td>\n",
       "      <td>106924551.0</td>\n",
       "      <td>172648.0</td>\n",
       "      <td>177169.0</td>\n",
       "      <td>53143.0</td>\n",
       "      <td>55830.0</td>\n",
       "      <td>2.691140e+15</td>\n",
       "      <td>2.226775e+14</td>\n",
       "      <td>3.271108e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <th>Abruzzo</th>\n",
       "      <td>729004305.0</td>\n",
       "      <td>729571151.0</td>\n",
       "      <td>2399.844823</td>\n",
       "      <td>2116294.0</td>\n",
       "      <td>2829792.0</td>\n",
       "      <td>84713582.0</td>\n",
       "      <td>84381441.0</td>\n",
       "      <td>29279339.0</td>\n",
       "      <td>29355363.0</td>\n",
       "      <td>104350340.0</td>\n",
       "      <td>105532420.0</td>\n",
       "      <td>148356.0</td>\n",
       "      <td>152254.0</td>\n",
       "      <td>45396.0</td>\n",
       "      <td>47662.0</td>\n",
       "      <td>2.647715e+15</td>\n",
       "      <td>2.187153e+14</td>\n",
       "      <td>3.189687e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <th>Abruzzo</th>\n",
       "      <td>706629792.0</td>\n",
       "      <td>707240933.0</td>\n",
       "      <td>2399.768004</td>\n",
       "      <td>2272802.0</td>\n",
       "      <td>2936730.0</td>\n",
       "      <td>79952197.0</td>\n",
       "      <td>79642735.0</td>\n",
       "      <td>28033160.0</td>\n",
       "      <td>28114250.0</td>\n",
       "      <td>98488811.0</td>\n",
       "      <td>99607902.0</td>\n",
       "      <td>112385.0</td>\n",
       "      <td>115241.0</td>\n",
       "      <td>33569.0</td>\n",
       "      <td>35454.0</td>\n",
       "      <td>2.735595e+15</td>\n",
       "      <td>2.249541e+14</td>\n",
       "      <td>3.108692e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-24</th>\n",
       "      <th>Veneto</th>\n",
       "      <td>225596711.0</td>\n",
       "      <td>225693675.0</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>534582.0</td>\n",
       "      <td>768383.0</td>\n",
       "      <td>7189438.0</td>\n",
       "      <td>7223154.0</td>\n",
       "      <td>8989396.0</td>\n",
       "      <td>9086800.0</td>\n",
       "      <td>24560829.0</td>\n",
       "      <td>24579468.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>21860.0</td>\n",
       "      <td>22754.0</td>\n",
       "      <td>1.227038e+12</td>\n",
       "      <td>1.239326e+11</td>\n",
       "      <td>1.148751e+06</td>\n",
       "      <td>406781583.0</td>\n",
       "      <td>406840893.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-25</th>\n",
       "      <th>Veneto</th>\n",
       "      <td>226892106.0</td>\n",
       "      <td>226990072.0</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>544697.0</td>\n",
       "      <td>778281.0</td>\n",
       "      <td>7290843.0</td>\n",
       "      <td>7323795.0</td>\n",
       "      <td>9044861.0</td>\n",
       "      <td>9143584.0</td>\n",
       "      <td>25162624.0</td>\n",
       "      <td>25185133.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>21276.0</td>\n",
       "      <td>22194.0</td>\n",
       "      <td>1.265358e+12</td>\n",
       "      <td>1.239708e+11</td>\n",
       "      <td>1.155531e+06</td>\n",
       "      <td>408781539.0</td>\n",
       "      <td>408834867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-26</th>\n",
       "      <th>Veneto</th>\n",
       "      <td>228933364.0</td>\n",
       "      <td>229031967.0</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>534915.0</td>\n",
       "      <td>771242.0</td>\n",
       "      <td>7482217.0</td>\n",
       "      <td>7517542.0</td>\n",
       "      <td>9205983.0</td>\n",
       "      <td>9307026.0</td>\n",
       "      <td>25735694.0</td>\n",
       "      <td>25752792.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>20782.0</td>\n",
       "      <td>21704.0</td>\n",
       "      <td>1.294111e+12</td>\n",
       "      <td>1.227355e+11</td>\n",
       "      <td>1.155699e+06</td>\n",
       "      <td>412255006.0</td>\n",
       "      <td>412310286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-27</th>\n",
       "      <th>Veneto</th>\n",
       "      <td>214647857.0</td>\n",
       "      <td>214744605.0</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>462945.0</td>\n",
       "      <td>684452.0</td>\n",
       "      <td>6555173.0</td>\n",
       "      <td>6585317.0</td>\n",
       "      <td>8301996.0</td>\n",
       "      <td>8401086.0</td>\n",
       "      <td>21873387.0</td>\n",
       "      <td>21885117.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13381.0</td>\n",
       "      <td>13830.0</td>\n",
       "      <td>1.317346e+12</td>\n",
       "      <td>1.129490e+11</td>\n",
       "      <td>1.054649e+06</td>\n",
       "      <td>386189271.0</td>\n",
       "      <td>386234427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-28</th>\n",
       "      <th>Veneto</th>\n",
       "      <td>203750066.0</td>\n",
       "      <td>203850901.0</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>498957.0</td>\n",
       "      <td>719187.0</td>\n",
       "      <td>5627962.0</td>\n",
       "      <td>5656738.0</td>\n",
       "      <td>7230392.0</td>\n",
       "      <td>7334436.0</td>\n",
       "      <td>19245821.0</td>\n",
       "      <td>19256627.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8825.0</td>\n",
       "      <td>9118.0</td>\n",
       "      <td>1.345221e+12</td>\n",
       "      <td>1.132375e+11</td>\n",
       "      <td>1.010849e+06</td>\n",
       "      <td>365976431.0</td>\n",
       "      <td>366018082.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7280 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     RRC_S_Succ    RRC_S_Att     RRC_S_SR  RRC_RE_Succ  \\\n",
       "Date       Regione                                                       \n",
       "2020-01-01 Abruzzo  722914109.0  723559625.0  2399.867376    2109077.0   \n",
       "2020-01-02 Abruzzo  718452155.0  718983916.0  2399.873855    2131711.0   \n",
       "2020-01-03 Abruzzo  732998734.0  733541088.0  2399.825795    2155915.0   \n",
       "2020-01-04 Abruzzo  729004305.0  729571151.0  2399.844823    2116294.0   \n",
       "2020-01-05 Abruzzo  706629792.0  707240933.0  2399.768004    2272802.0   \n",
       "...                         ...          ...          ...          ...   \n",
       "2021-02-24 Veneto   225596711.0  225693675.0  2400.000000     534582.0   \n",
       "2021-02-25 Veneto   226892106.0  226990072.0  2400.000000     544697.0   \n",
       "2021-02-26 Veneto   228933364.0  229031967.0  2400.000000     534915.0   \n",
       "2021-02-27 Veneto   214647857.0  214744605.0  2400.000000     462945.0   \n",
       "2021-02-28 Veneto   203750066.0  203850901.0  2400.000000     498957.0   \n",
       "\n",
       "                    RRC_RE_Att  IntraF_Hout_Succ  IntraF_Hout_Att  \\\n",
       "Date       Regione                                                  \n",
       "2020-01-01 Abruzzo   2755617.0        69768501.0       69410898.0   \n",
       "2020-01-02 Abruzzo   2792486.0        84230949.0       83904966.0   \n",
       "2020-01-03 Abruzzo   2846211.0        86249305.0       85912857.0   \n",
       "2020-01-04 Abruzzo   2829792.0        84713582.0       84381441.0   \n",
       "2020-01-05 Abruzzo   2936730.0        79952197.0       79642735.0   \n",
       "...                        ...               ...              ...   \n",
       "2021-02-24 Veneto     768383.0         7189438.0        7223154.0   \n",
       "2021-02-25 Veneto     778281.0         7290843.0        7323795.0   \n",
       "2021-02-26 Veneto     771242.0         7482217.0        7517542.0   \n",
       "2021-02-27 Veneto     684452.0         6555173.0        6585317.0   \n",
       "2021-02-28 Veneto     719187.0         5627962.0        5656738.0   \n",
       "\n",
       "                    InterF_Hout_Succ  InterF_Hout_Att     Handover  \\\n",
       "Date       Regione                                                   \n",
       "2020-01-01 Abruzzo        29058705.0       29152448.0   87240956.0   \n",
       "2020-01-02 Abruzzo        28343453.0       28429081.0  103161979.0   \n",
       "2020-01-03 Abruzzo        29106383.0       29189094.0  105738844.0   \n",
       "2020-01-04 Abruzzo        29279339.0       29355363.0  104350340.0   \n",
       "2020-01-05 Abruzzo        28033160.0       28114250.0   98488811.0   \n",
       "...                              ...              ...          ...   \n",
       "2021-02-24 Veneto          8989396.0        9086800.0   24560829.0   \n",
       "2021-02-25 Veneto          9044861.0        9143584.0   25162624.0   \n",
       "2021-02-26 Veneto          9205983.0        9307026.0   25735694.0   \n",
       "2021-02-27 Veneto          8301996.0        8401086.0   21873387.0   \n",
       "2021-02-28 Veneto          7230392.0        7334436.0   19245821.0   \n",
       "\n",
       "                        Hin_Att  InterR_HO_OUT_E2W_Succ  \\\n",
       "Date       Regione                                        \n",
       "2020-01-01 Abruzzo   88551230.0                106931.0   \n",
       "2020-01-02 Abruzzo  104312783.0                166524.0   \n",
       "2020-01-03 Abruzzo  106924551.0                172648.0   \n",
       "2020-01-04 Abruzzo  105532420.0                148356.0   \n",
       "2020-01-05 Abruzzo   99607902.0                112385.0   \n",
       "...                         ...                     ...   \n",
       "2021-02-24 Veneto    24579468.0                   195.0   \n",
       "2021-02-25 Veneto    25185133.0                   195.0   \n",
       "2021-02-26 Veneto    25752792.0                   195.0   \n",
       "2021-02-27 Veneto    21885117.0                   195.0   \n",
       "2021-02-28 Veneto    19256627.0                   195.0   \n",
       "\n",
       "                    InterR_HO_OUT_E2W_Att  InterR_HO_OUT_E2G_Succ  \\\n",
       "Date       Regione                                                  \n",
       "2020-01-01 Abruzzo               109647.0                 39290.0   \n",
       "2020-01-02 Abruzzo               171219.0                 51450.0   \n",
       "2020-01-03 Abruzzo               177169.0                 53143.0   \n",
       "2020-01-04 Abruzzo               152254.0                 45396.0   \n",
       "2020-01-05 Abruzzo               115241.0                 33569.0   \n",
       "...                                   ...                     ...   \n",
       "2021-02-24 Veneto                   196.0                 21860.0   \n",
       "2021-02-25 Veneto                   196.0                 21276.0   \n",
       "2021-02-26 Veneto                   196.0                 20782.0   \n",
       "2021-02-27 Veneto                     5.0                 13381.0   \n",
       "2021-02-28 Veneto                     6.0                  8825.0   \n",
       "\n",
       "                    InterR_HO_OUT_E2G_Att  Download vol.   Upload vol.  \\\n",
       "Date       Regione                                                       \n",
       "2020-01-01 Abruzzo                41672.0   2.892451e+15  2.471538e+14   \n",
       "2020-01-02 Abruzzo                54111.0   2.675922e+15  2.237421e+14   \n",
       "2020-01-03 Abruzzo                55830.0   2.691140e+15  2.226775e+14   \n",
       "2020-01-04 Abruzzo                47662.0   2.647715e+15  2.187153e+14   \n",
       "2020-01-05 Abruzzo                35454.0   2.735595e+15  2.249541e+14   \n",
       "...                                   ...            ...           ...   \n",
       "2021-02-24 Veneto                 22754.0   1.227038e+12  1.239326e+11   \n",
       "2021-02-25 Veneto                 22194.0   1.265358e+12  1.239708e+11   \n",
       "2021-02-26 Veneto                 21704.0   1.294111e+12  1.227355e+11   \n",
       "2021-02-27 Veneto                 13830.0   1.317346e+12  1.129490e+11   \n",
       "2021-02-28 Veneto                  9118.0   1.345221e+12  1.132375e+11   \n",
       "\n",
       "                          #Users  ERAB_S_Succ   ERAB_S_Att  \n",
       "Date       Regione                                          \n",
       "2020-01-01 Abruzzo  3.184160e+06          NaN          NaN  \n",
       "2020-01-02 Abruzzo  3.220258e+06          NaN          NaN  \n",
       "2020-01-03 Abruzzo  3.271108e+06          NaN          NaN  \n",
       "2020-01-04 Abruzzo  3.189687e+06          NaN          NaN  \n",
       "2020-01-05 Abruzzo  3.108692e+06          NaN          NaN  \n",
       "...                          ...          ...          ...  \n",
       "2021-02-24 Veneto   1.148751e+06  406781583.0  406840893.0  \n",
       "2021-02-25 Veneto   1.155531e+06  408781539.0  408834867.0  \n",
       "2021-02-26 Veneto   1.155699e+06  412255006.0  412310286.0  \n",
       "2021-02-27 Veneto   1.054649e+06  386189271.0  386234427.0  \n",
       "2021-02-28 Veneto   1.010849e+06  365976431.0  366018082.0  \n",
       "\n",
       "[7280 rows x 20 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_traffic_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothen with Kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Kalman Filter to traffic features\n",
    "dict_kalman = {}\n",
    "path_traffic_predictions=\"{}predictions/traffic.pkl\".format(saved)\n",
    "if recompute_kalman:\n",
    "    for trafficKPI in trafficKPIsPrecompute:\n",
    "        #current_df_kalman = pd.DataFrame({\"{}_smoothened\".format(trafficKPI): []})\n",
    "        dfs_current_kpi = []\n",
    "        for region in regions:\n",
    "            kf = KalmanFilter(transition_matrices = [1],\n",
    "                      observation_matrices = [1],\n",
    "                      initial_state_mean = 0,\n",
    "                      initial_state_covariance = 1,\n",
    "                      observation_covariance=1,\n",
    "                      transition_covariance=.05)\n",
    "\n",
    "            series = df_traffic_predictions.xs(region, level=1)[trafficKPI]\n",
    "            # t = 21/03 -> current_series = series[:t] -> .em(current_series) -> features_for_day_t = smooth(current_series)\n",
    "            kf = kf.em(series)                                           # 01/03/20 -> 31/01/21\n",
    "            (smoothened, smoothed_state_covariances) = kf.smooth(series) # 01/03/20 -> 31/01/21\n",
    "            df_region_kpi = pd.DataFrame({\"noisy\": series})\n",
    "            df_region_kpi['smooth'] = smoothened.squeeze()\n",
    "            df_region_kpi['Regione'] = region\n",
    "            df_region_kpi.reset_index(inplace=True)\n",
    "            df_region_kpi.set_index(['Date', 'Regione'], inplace=True)\n",
    "            dfs_current_kpi.append(df_region_kpi)\n",
    "\n",
    "            dict_kalman[\"{}_{}\".format(trafficKPI, region)] = kf\n",
    "\n",
    "        df_traffic_predictions[\"{}_kf\".format(trafficKPI)] = pd.concat(dfs_current_kpi)['smooth']\n",
    "        df_traffic_predictions.to_pickle(path_traffic_predictions)\n",
    "else:\n",
    "    pass\n",
    "    #df_traffic_predictions = pd.read_pickle(path_traffic_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply MA to traffic features\n",
    "path_traffic_predictions=\"{}predictions/traffic.pkl\".format(saved)\n",
    "if recompute_MA:\n",
    "    for trafficKPI in trafficKPIsPrecompute:\n",
    "        #current_df_kalman = pd.DataFrame({\"{}_smoothened\".format(trafficKPI): []})\n",
    "        dfs_current_kpi = []\n",
    "        for region in regions:\n",
    "            series = df_traffic_predictions.xs(region, level=1)[trafficKPI]\n",
    "            # t = 21/03 -> current_series = series[:t] -> .em(current_series) -> features_for_day_t = smooth(current_series)\n",
    "            rolling_amount = 7\n",
    "            series_ma = series.rolling(rolling_amount).mean()\n",
    "            \n",
    "            series_ma = series_ma.loc[(series_ma.index<start_date_unavailable)|(series_ma.index>=end_date_unavailable+pd.Timedelta(days=rolling_amount))]\n",
    "            \n",
    "            df_region_kpi = pd.DataFrame({\"noisy\": series})\n",
    "            df_region_kpi['smooth'] = series_ma\n",
    "            df_region_kpi['Regione'] = region\n",
    "            df_region_kpi.reset_index(inplace=True)\n",
    "            df_region_kpi.set_index(['Date', 'Regione'], inplace=True)\n",
    "            dfs_current_kpi.append(df_region_kpi)\n",
    "\n",
    "        df_traffic_predictions[\"{}_MA\".format(trafficKPI)] = pd.concat(dfs_current_kpi)['smooth']\n",
    "        df_traffic_predictions.to_pickle(path_traffic_predictions)\n",
    "else:\n",
    "    df_traffic_predictions = pd.read_pickle(path_traffic_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario_input: both, only_traffic, only_traffic_no_mobility, only_covid\n",
    "scenario_input = \"both\"\n",
    "is_both, is_only_traffic, is_no_mobility, is_only_covid = \\\n",
    "    scenario_input == 'both', scenario_input == 'only_traffic', \\\n",
    "    scenario_input == 'only_traffic_no_mobility', scenario_input == 'only_covid'\n",
    "assert is_both or is_only_traffic or is_no_mobility or is_only_covid, \"Wrong\"\n",
    "trafficKPIs, covidKPIs = [], []\n",
    "if is_both:\n",
    "    trafficKPIs = [col for col in df_traffic_predictions.columns if \"MA\" in col]\n",
    "    covidKPIs = [col for col in df_covid_predictions.columns if \"mean\" in col]\n",
    "elif is_only_traffic:\n",
    "    trafficKPIs = [col for col in df_traffic_predictions.columns if \"MA\" in col]\n",
    "elif is_no_mobility:\n",
    "    trafficKPIs = [col for col in df_traffic_predictions.columns if \"MA\" in col and \"over\" not in col and \"sers\" not in col]\n",
    "elif is_only_covid:\n",
    "    covidKPIs = [col for col in df_covid_predictions.columns if \"mean\" in col]\n",
    "temperatureKPIs = []#[col for col in df_temperature.columns if \"min\" in col]\n",
    "targetCovid = ['R_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Handover_MA', 'Download vol._MA', 'Upload vol._MA', '#Users_MA']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trafficKPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R_mean']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covidKPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_prediction(range_dates, min_farsightness, farsightness, delta_features, step_target):\n",
    "    lags = range(delta_features)\n",
    "    lags_target = range(min_farsightness, farsightness, step_target)\n",
    "    all_dfs = []\n",
    "    # prima ondata\n",
    "    for region in regions_to_train:\n",
    "        # filter ts by region\n",
    "        df_traffic_ts = df_traffic_predictions.loc[(df_traffic_predictions.index.get_level_values(1)==region), trafficKPIs].copy()\n",
    "        df_covid_ts = df_covid_predictions.loc[df_covid_predictions.index.get_level_values(1)==region, list(set(covidKPIs+targetCovid))].copy()\n",
    "        #df_temperature_ts = df_temperature.loc[df_temperature.index.get_level_values(1)==region, temperatureKPIs].copy()\n",
    "        \n",
    "        df_traffic_ts = df_traffic_ts.groupby(level=1).transform(lambda x: (x-x.mean())/x.std(ddof=1))\n",
    "        #df_temperature_ts = df_temperature_ts.groupby(level=1).transform(lambda x: (x-x.mean())/x.std(ddof=1))\n",
    "\n",
    "        df_covid_ts.reset_index(inplace=True)\n",
    "        #df_temperature_ts.reset_index(inplace=True)\n",
    "        df_traffic_ts.reset_index(inplace=True)\n",
    "\n",
    "        df_covid_ts = df_covid_ts.set_index('Date')\n",
    "        #df_temperature_ts = df_temperature_ts.set_index('Date')\n",
    "        df_traffic_ts = df_traffic_ts.set_index('Date')\n",
    "\n",
    "        df_ts = pd.DataFrame()\n",
    "        features = []\n",
    "        targets = []\n",
    "        target_col = targetCovid[0]\n",
    "        df_target_ts = df_covid_ts.copy()\n",
    "\n",
    "        train_dates_intersection = df_traffic_ts.index.intersection(df_covid_ts.index)\n",
    "\n",
    "        train_dates = []\n",
    "        #if len(temperatureKPIs) > 0:\n",
    "        #    train_dates_intersection = train_dates_intersection.isin(df_temperature_ts.index)\n",
    "        for date_val in train_dates_intersection:\n",
    "            if any(date_val in x for x in range_dates):\n",
    "                train_dates.append(date_val)\n",
    "\n",
    "        train_dates = pd.to_datetime(train_dates)\n",
    "\n",
    "        df_covid_ts, df_traffic_ts = df_covid_ts.loc[df_covid_ts.index.isin(train_dates)], df_traffic_ts.loc[df_traffic_ts.index.isin(train_dates)]\n",
    "        #df_temperature_ts = df_temperature_ts.loc[df_temperature_ts.index.isin(train_dates)]\n",
    "        for lag in lags_target:\n",
    "            target = \"target_{}\".format(lag)\n",
    "            targets.append(target)\n",
    "            df_ts[target] = df_target_ts.copy().shift(-1*lag)[target_col]\n",
    "        \n",
    "        # use also today feature\n",
    "        use_today_feature = True\n",
    "        if use_today_feature:\n",
    "            if len(trafficKPIs) > 0:\n",
    "                df_ts[trafficKPIs] = df_traffic_ts[trafficKPIs].copy()\n",
    "                features += trafficKPIs\n",
    "            if len(covidKPIs) > 0:\n",
    "                df_ts[covidKPIs] = df_covid_ts[covidKPIs].copy()\n",
    "                features += covidKPIs\n",
    "        \n",
    "        for lag in lags:\n",
    "            lag_shift = lag+1\n",
    "            for col in trafficKPIs:\n",
    "                feature = \"{}_{}\".format(col, lag_shift)\n",
    "                #print(feature, df_traffic_ts.shift(lag_shift).loc[:, col])\n",
    "                df_ts[feature] = df_traffic_ts.copy().shift(lag_shift).loc[:, col]\n",
    "                features.append(feature)\n",
    "            for col in covidKPIs:\n",
    "                feature = \"{}_{}\".format(col, lag_shift)\n",
    "                df_ts[feature] = df_covid_ts.copy().shift(lag_shift)[col]\n",
    "                features.append(feature)\n",
    "            for col in temperatureKPIs:\n",
    "                feature = \"{}_{}\".format(col, lag_shift)\n",
    "                df_ts[feature] = df_temperature_ts.copy().shift(lag_shift)[col]\n",
    "                features.append(feature)\n",
    "\n",
    "        df_ts = df_ts[targets+features]\n",
    "        df_ts.dropna(subset=features, inplace=True)\n",
    "        df_ts['Regione'] = region\n",
    "        df_ts = df_ts.reset_index().set_index(['Date', 'Regione'])\n",
    "        all_dfs.append(df_ts.copy())\n",
    "    return pd.concat(all_dfs), targets, features, lags, lags_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "divider_po, divider_so = pd.to_datetime('2020-07-01'), pd.to_datetime('2020-10-01')\n",
    "start_train_po, end_train_so = pd.to_datetime('2020-03-01'), pd.to_datetime('2020-10-25')\n",
    "start_po = start_train_po\n",
    "ranges_train_PO = [pd.date_range(start_po, divider_po)]\n",
    "ranges_train_SO = [pd.date_range(divider_so, end_train_so)]\n",
    "last_date_with_unseen = min(df_traffic_predictions.index.get_level_values(0).max(), df_unseen_covid.index.get_level_values(0).max())\n",
    "\n",
    "last_date = min(last_date_with_unseen, capped_last_date)\n",
    "\n",
    "ranges_unseen = [pd.date_range(last_date, last_date_with_unseen)]\n",
    "\n",
    "ranges_so = [pd.date_range(divider_so, last_date)]\n",
    "regions_to_train = [\"Lombardia\", \"Campania\", \"Puglia\", \"Lazio\", \"Sardegna\", \"Veneto\"]#regions\n",
    "\n",
    "min_farsightness = 1\n",
    "farsightness = 35\n",
    "delta_features = 21\n",
    "step_target = 1\n",
    "\n",
    "ranges_test_SO = [pd.date_range(end_train_so-pd.Timedelta(days=delta_features), last_date_with_unseen)]\n",
    "\n",
    "(df_train_prediction_PO, targets, features, lags, lags_target), (df_train_prediction_SO, _, _, _, _) = \\\n",
    "    build_df_prediction(ranges_train_PO, min_farsightness, farsightness, delta_features, step_target), \\\n",
    "    build_df_prediction(ranges_train_SO, min_farsightness, farsightness, delta_features, step_target)\n",
    "\n",
    "(df_test_prediction, _, _, _, _) = build_df_prediction(ranges_test_SO, min_farsightness, farsightness, delta_features, step_target)\n",
    "df_train_prediction = pd.concat([df_train_prediction_PO, df_train_prediction_SO])\n",
    "\n",
    "df_unseen_prediction = df_test_prediction.loc[df_test_prediction.index.get_level_values('Date')>capped_last_date]\n",
    "df_test_prediction = df_test_prediction.loc[df_test_prediction.index.get_level_values('Date')<=capped_last_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prediction.xs('Lombardia', level='Regione').to_csv(\"{}predictions/train.csv\".format(saved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_prediction.xs('Lombardia', level='Regione').to_csv(\"{}predictions/test.csv\".format(saved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2021-02-28 00:00:00'), NaT)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_prediction.index.get_level_values('Date').max(), df_unseen_prediction.index.get_level_values('Date').min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {'objective': 'reg:squarederror'}\n",
    "\n",
    "is_xgb = False\n",
    "is_polynomial = False\n",
    "is_lstm = True\n",
    "\n",
    "features_without_lag = trafficKPIs+covidKPIs+temperatureKPIs\n",
    "num_features_t = len(features_without_lag)\n",
    "n_timesteps = len(lags)+1\n",
    "\n",
    "def build_mlp(n_timesteps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def build_lstm_1(n_timesteps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def build_lstm_2(n_timesteps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def build_lstm_3(n_timesteps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(RepeatVector(1))\n",
    "    model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(32, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def build_lstm_4(n_timesteps, n_features, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(32, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "models_constructors = {\n",
    "    \"Poly1\": partial(make_pipeline, PolynomialFeatures(1), Ridge()),\n",
    "    \"Poly2\": partial(make_pipeline, PolynomialFeatures(2), Ridge()),\n",
    "    \"Poly3\": partial(make_pipeline, PolynomialFeatures(3), Ridge()),\n",
    "    #\"RandomForest\": RandomForestRegressor,\n",
    "    \"XGBoost\": XGBRegressor,\n",
    "    #\"EncDecLSTM\": partial(build_lstm_3, n_timesteps, num_features_t)\n",
    "}\n",
    "\n",
    "def build_model(model_name):\n",
    "    is_xgb, is_poly3, is_poly2, is_rf, is_lstm = model_name==\"XGBoost\", model_name==\"Poly3\", model_name==\"Poly2\", model_name==\"RandomForest\", model_name==\"EncDecLSTM\"\n",
    "\n",
    "    if is_poly2:\n",
    "        return make_pipeline(PolynomialFeatures(2), Ridge())\n",
    "    elif is_poly3:\n",
    "        return make_pipeline(PolynomialFeatures(3), Ridge())\n",
    "    elif is_rf:\n",
    "        return RandomForestRegressor()\n",
    "    elif is_xgb:\n",
    "        return XGBRegressor()\n",
    "    elif is_lstm:\n",
    "        return build_lstm_3(n_timesteps, num_features_t)\n",
    "    else:\n",
    "        return make_pipeline(PolynomialFeatures(1), Ridge())\n",
    "\n",
    "\n",
    "training_time = {m: {r:{} for r in regions_to_train} for m in models_constructors.keys()}  \n",
    "\n",
    "def train_in_interval(interval):\n",
    "    models_regions = {m: {r:{} for r in regions_to_train} for m in models_constructors.keys()}\n",
    "    \n",
    "    # start training\n",
    "    for region in regions_to_train:\n",
    "        df_ts = df_train_prediction.loc[df_train_prediction.index.get_level_values(1)==region]\n",
    "        df_ts = df_ts.reset_index().set_index('Date').drop(columns='Regione')\n",
    "        df_ts = df_ts.loc[(df_ts.index>=interval[0])&(df_ts.index < interval[1])]\n",
    "        for model_name in models_constructors.keys():\n",
    "            for lag in lags_target:\n",
    "                start_training_time = time.perf_counter()\n",
    "                target = \"target_{}\".format(lag)\n",
    "                df_ts_lag = df_ts.copy().drop(columns=[col for col in targets if (col not in features) and (col != target)]).dropna()\n",
    "                #print(\"{} for {} with lag {}: {} -> {}\".format(model_name, region, lag, min(interval), max(interval)))\n",
    "                is_lstm = \"lstm\" in model_name.lower()\n",
    "                model = build_model(model_name)\n",
    "                if is_lstm:\n",
    "                    callback = EarlyStopping(monitor='loss', patience=3)\n",
    "                    # N x T x F\n",
    "                    lstm_input = df_ts_lag[features].to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "                    model.fit(lstm_input, df_ts_lag[target], epochs=100, batch_size=32, verbose=0, callbacks=[callback])\n",
    "                    models_regions[model_name][region][lag] = model\n",
    "                else:\n",
    "                    model.fit(df_ts_lag[features].values, df_ts_lag[target])\n",
    "                    models_regions[model_name][region][lag] = model\n",
    "                new_time = time.perf_counter()\n",
    "                training_time[model_name][region][lag] = new_time - start_training_time\n",
    "    return models_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train = True\n",
    "\n",
    "if do_train:\n",
    "    models_regions = train_in_interval((start_train_po, end_train_so))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models_constructors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0bec0a5192f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_ts_test_region\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwfval_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregions_to_train\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_constructors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_in_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models_constructors' is not defined"
     ]
    }
   ],
   "source": [
    "train_dates_region_every_n = {}\n",
    "test_dates_region_every_n = {}\n",
    "train_dates_region = {}\n",
    "test_dates_region = {}\n",
    "df_ts_test_region = {}\n",
    "\n",
    "wfval_time = {m: {r:{} for r in regions_to_train} for m in models_constructors.keys()}\n",
    "\n",
    "def test_in_interval(interval):\n",
    "    #print(\"TEST START\")\n",
    "                \n",
    "    df_results = pd.DataFrame() # lag, region, prediction, target; index = date\n",
    "    results_dict = []\n",
    "\n",
    "    for region in regions_to_train:\n",
    "        df_ts = df_test_prediction.loc[df_test_prediction.index.get_level_values(1)==region]\n",
    "        df_ts = df_ts.reset_index().set_index('Date').drop(columns='Regione')\n",
    "        df_ts = df_ts.loc[(df_ts.index>=interval[0])&(df_ts.index < interval[1])]\n",
    "        first_date, last_date = df_ts.index.min(), df_ts.index.max()\n",
    "        current_region_values = []\n",
    "        \n",
    "        test_dates_region[region] = pd.date_range(first_date, last_date)\n",
    "        df_ts_test_region[region] = df_ts\n",
    "        test_dates = test_dates_region[region].unique()\n",
    "        assert test_dates_region[region].shape[0]==len(test_dates), \"Something wrong\"\n",
    "        for model_name in models_constructors.keys():\n",
    "            for idx_lag, lag in enumerate(lags_target):\n",
    "                start_wfval_time = time.perf_counter()\n",
    "                target_col = targets[idx_lag]\n",
    "                #walk_forward_df = df_train_prediction.copy().drop(columns=[col for col in targets if (col not in features) and (col != target_col)]).dropna()\n",
    "                \n",
    "                current_df_train_prediction = df_train_prediction.copy().xs(region, level='Regione')\n",
    "                walk_forward_df = current_df_train_prediction.drop(columns=[col for col in targets if (col not in features) and (col != target_col)]).dropna()\n",
    "            \n",
    "                current_df_ts = df_ts[features+[target_col]].copy().dropna()\n",
    "                test_dates = current_df_ts.index\n",
    "                print(\"{} for {} with lag = {}: {} -> {}, {}\".format(model_name, region, lag, min(test_dates), max(test_dates), current_df_ts.shape))\n",
    "                for i, t in enumerate(test_dates):\n",
    "                    current_df_ts = df_ts.loc[t:t+datetime.timedelta(days=0)]\n",
    "                    X_test_ts = current_df_ts[features]\n",
    "                    \n",
    "                    is_lstm = \"lstm\" in model_name.lower()\n",
    "                    \n",
    "                    if is_lstm:\n",
    "                        lstm_input = X_test_ts.to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "                        predictions = models_regions[model_name][region][lag].predict(lstm_input).flatten()\n",
    "                    else:\n",
    "                        predictions = models_regions[model_name][region][lag].predict(X_test_ts.values)\n",
    "\n",
    "                    X_test_fit, y_test_fit = X_test_ts, current_df_ts[target_col].values\n",
    "\n",
    "                    #current_result = {\"date\": t + datetime.timedelta(days=lag), \"lag\": lag, \"region\": region, \"prediction\": predictions[0], \"target\": y_test_fit}\n",
    "                    current_result = {\"model\": model_name,\"date\": t + datetime.timedelta(days=lag), \"lag\": lag, \"region\": region, \"prediction\": predictions[0], \"target\": y_test_fit[0]}\n",
    "                    results_dict.append(current_result)\n",
    "\n",
    "                    walk_forward_df = walk_forward_df.append(current_df_ts)\n",
    "                    walk_forward_df = walk_forward_df.sort_index()\n",
    "                    window_train = 45\n",
    "                    \n",
    "                    if is_lstm:\n",
    "                        callback = EarlyStopping(monitor='loss', patience=3)\n",
    "                        window_train = 32\n",
    "                        # I keep the last model and update weights using a window (last 30 values)\n",
    "                        model = models_regions[model_name][region][lag]\n",
    "                        lstm_input = walk_forward_df[features].to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "                        lstm_input = lstm_input[-1*window_train:]\n",
    "                        lstm_target = walk_forward_df[target_col].values[-1*window_train:]\n",
    "                        model.fit(lstm_input, lstm_target, epochs=10, batch_size=8, verbose=0, callbacks=[callback])\n",
    "                        models_regions[model_name][region][lag] = model\n",
    "                    else:\n",
    "                        use_window = True\n",
    "                        if use_window:\n",
    "                            window_walk_forward_df = walk_forward_df.iloc[-1*window_train:]\n",
    "                            features_vals, targets_vals = window_walk_forward_df[features], window_walk_forward_df[target_col]\n",
    "                            #features_vals, targets_vals = walk_forward_df[features].iloc[-1*window_train:], walk_forward_df[target_col].iloc[-1*window_train:]\n",
    "                            #print(features_vals.shape, targets_vals.shape)\n",
    "                            models_regions[model_name][region][lag] = build_model(model_name)\n",
    "                            models_regions[model_name][region][lag].fit(features_vals.values, targets_vals)\n",
    "                        else:\n",
    "                            # here I should do the same as before, for now not trying because I suppose it's better (to more enphasize recent behaviour)\n",
    "                            pass\n",
    "                new_time = time.perf_counter()\n",
    "                wfval_time[model_name][region][lag] = new_time - start_wfval_time\n",
    "    df_results = pd.DataFrame(results_dict)\n",
    "    df_results = df_results.dropna()\n",
    "    df_results.set_index(['model', 'date', 'region', 'lag'], inplace=True)\n",
    "    df_results['error']=(df_results['prediction']-df_results['target']).abs()\n",
    "    df_results['error_2'] = df_results['error']**2\n",
    "    return models_regions, df_results\n",
    "\n",
    "min_date, max_date = df_test_prediction.index.get_level_values(0).min(), df_test_prediction.index.get_level_values(0).max()\n",
    "if do_train:\n",
    "    models_regions, df_results = test_in_interval((min_date, max_date))\n",
    "else:\n",
    "    path_results = \"{}predictions/results_v3.csv\".format(saved)\n",
    "    df_results = pd.read_csv(path_results)\n",
    "    df_results['date'] = pd.to_datetime(df_results['date'])\n",
    "    df_results.set_index(['model', 'date', 'region', 'lag'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>error</th>\n",
       "      <th>error_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>lag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Poly1</th>\n",
       "      <th>2020-10-30</th>\n",
       "      <th>Lombardia</th>\n",
       "      <th>1</th>\n",
       "      <td>0.900220</td>\n",
       "      <td>1.060701</td>\n",
       "      <td>0.160481</td>\n",
       "      <td>0.025754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-31</th>\n",
       "      <th>Lombardia</th>\n",
       "      <th>1</th>\n",
       "      <td>0.996333</td>\n",
       "      <td>1.035190</td>\n",
       "      <td>0.038857</td>\n",
       "      <td>0.001510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-01</th>\n",
       "      <th>Lombardia</th>\n",
       "      <th>1</th>\n",
       "      <td>0.986205</td>\n",
       "      <td>1.009310</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.000534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02</th>\n",
       "      <th>Lombardia</th>\n",
       "      <th>1</th>\n",
       "      <td>0.973615</td>\n",
       "      <td>0.985876</td>\n",
       "      <td>0.012261</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <th>Lombardia</th>\n",
       "      <th>1</th>\n",
       "      <td>0.964463</td>\n",
       "      <td>0.968252</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">XGBoost</th>\n",
       "      <th>2021-02-13</th>\n",
       "      <th>Veneto</th>\n",
       "      <th>34</th>\n",
       "      <td>1.166980</td>\n",
       "      <td>1.191350</td>\n",
       "      <td>0.024371</td>\n",
       "      <td>0.000594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-14</th>\n",
       "      <th>Veneto</th>\n",
       "      <th>34</th>\n",
       "      <td>1.132433</td>\n",
       "      <td>1.208873</td>\n",
       "      <td>0.076440</td>\n",
       "      <td>0.005843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-15</th>\n",
       "      <th>Veneto</th>\n",
       "      <th>34</th>\n",
       "      <td>1.140391</td>\n",
       "      <td>1.228522</td>\n",
       "      <td>0.088132</td>\n",
       "      <td>0.007767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-16</th>\n",
       "      <th>Veneto</th>\n",
       "      <th>34</th>\n",
       "      <td>1.210197</td>\n",
       "      <td>1.251320</td>\n",
       "      <td>0.041122</td>\n",
       "      <td>0.001691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-17</th>\n",
       "      <th>Veneto</th>\n",
       "      <th>34</th>\n",
       "      <td>1.192842</td>\n",
       "      <td>1.268021</td>\n",
       "      <td>0.075179</td>\n",
       "      <td>0.005652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63648 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  prediction    target     error   error_2\n",
       "model   date       region    lag                                          \n",
       "Poly1   2020-10-30 Lombardia 1      0.900220  1.060701  0.160481  0.025754\n",
       "        2020-10-31 Lombardia 1      0.996333  1.035190  0.038857  0.001510\n",
       "        2020-11-01 Lombardia 1      0.986205  1.009310  0.023105  0.000534\n",
       "        2020-11-02 Lombardia 1      0.973615  0.985876  0.012261  0.000150\n",
       "        2020-11-03 Lombardia 1      0.964463  0.968252  0.003789  0.000014\n",
       "...                                      ...       ...       ...       ...\n",
       "XGBoost 2021-02-13 Veneto    34     1.166980  1.191350  0.024371  0.000594\n",
       "        2021-02-14 Veneto    34     1.132433  1.208873  0.076440  0.005843\n",
       "        2021-02-15 Veneto    34     1.140391  1.228522  0.088132  0.007767\n",
       "        2021-02-16 Veneto    34     1.210197  1.251320  0.041122  0.001691\n",
       "        2021-02-17 Veneto    34     1.192842  1.268021  0.075179  0.005652\n",
       "\n",
       "[63648 rows x 4 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df_results_groupped(df_results, col_prediction = 'prediction', col_error = 'error_2', col_rmse = 'rmse', col_r2 = 'r2', col_mape = 'mape'):\n",
    "    try:\n",
    "        groupped_df = df_results.groupby(level=['model', 'region', 'lag'])\n",
    "    except:\n",
    "        print(\"WARNING: not groupped\")\n",
    "        groupped_df = df_results\n",
    "    df = pd.DataFrame()\n",
    "    df[col_rmse] = np.sqrt(groupped_df[col_error].mean())\n",
    "    df[col_r2]=groupped_df.apply(lambda g: r2_score( g[col_prediction], g['target'] ))\n",
    "    df[col_mape] = groupped_df.apply(lambda g: np.mean(np.abs((g['target'] - g['prediction']) / g['target'])) * 100)\n",
    "    return df[[col_rmse, col_r2, col_mape]]\n",
    "\n",
    "df_results_mean = build_df_results_groupped(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results...\n",
      "Saving RMSE, R2, MAPE...\n",
      "Saving time of execution...\n",
      "Saving models...\n",
      "Saving datasets...\n",
      "OK!\n"
     ]
    }
   ],
   "source": [
    "name_current_save = 'results_v1_{}_{}'.format(delta_features, scenario_input)\n",
    "# Tf = 1-days. Only both. OK\n",
    "# Tf = 7-days. Only both. OK\n",
    "# Tf = 14-days. Can be both, only_traffic, only_traffic_no_mobility, only_covid. OK\n",
    "# Tf = 21-days. Only both. OK\n",
    "path_results_to_save = \"{}predictions/{}\".format(saved, name_current_save)\n",
    "path_results_to_save_models = \"{}models/{}\".format(saved, name_current_save)\n",
    "path_results_to_mean_save = \"{}_mean\".format(path_results_to_save)\n",
    "path_results_to_model_save = \"{}_models\".format(path_results_to_save_models)\n",
    "path_results_to_save_train = \"{}_df_train\".format(path_results_to_save)\n",
    "path_results_to_save_test = \"{}_df_test\".format(path_results_to_save)\n",
    "path_results_to_save_unseen = \"{}_df_unseen\".format(path_results_to_save)\n",
    "path_results_to_time_save = \"{}_time\".format(path_results_to_save)\n",
    "print(\"Saving results...\")\n",
    "df_results.to_csv(\"{}.csv\".format(path_results_to_save))\n",
    "print(\"Saving RMSE, R2, MAPE...\")\n",
    "df_results_mean.to_csv(\"{}.csv\".format(path_results_to_mean_save))\n",
    "print(\"Saving time of execution...\")\n",
    "#df_results_mean.to_csv(\"{}.csv\".format(path_results_to_mean_save))\n",
    "\n",
    "with open(\"{}_train\".format(path_results_to_time_save), 'wb') as handle:\n",
    "    pickle.dump(training_time, handle)\n",
    "    \n",
    "with open(\"{}_wfval\".format(path_results_to_time_save), 'wb') as handle:\n",
    "    pickle.dump(wfval_time, handle)\n",
    "    \n",
    "    \n",
    "print(\"Saving models...\")\n",
    "for model_name in models_regions.keys():\n",
    "    for region_name in models_regions[model_name].keys():\n",
    "        for lag in models_regions[model_name][region_name].keys():\n",
    "            current_models_dir = '{}/{}/{}/'.format(path_results_to_model_save, model_name, region_name)\n",
    "            model_path = '{}lag_{}.pkl'.format(current_models_dir, lag)\n",
    "            try:\n",
    "                os.makedirs(current_models_dir)\n",
    "            except:\n",
    "                pass\n",
    "            current_model = models_regions[model_name][region_name][lag]\n",
    "            if \"LSTM\" in model_name:\n",
    "                try:\n",
    "                    os.makedirs(current_models_dir)\n",
    "                except:\n",
    "                    pass\n",
    "                current_model.save(\"{}lag_{}\".format(current_models_dir, lag))\n",
    "            else:\n",
    "                with open(model_path, 'wb') as handle:\n",
    "                    pickle.dump(current_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# TODO salvare i dataset train, test, unseen\n",
    "print(\"Saving datasets...\")\n",
    "df_train_prediction.to_csv(\"{}.csv\".format(path_results_to_save_train))\n",
    "df_test_prediction.to_csv(\"{}.csv\".format(path_results_to_save_test))\n",
    "df_unseen_prediction.to_csv(\"{}.csv\".format(path_results_to_save_unseen))\n",
    "\n",
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Piemonte'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-d9403fded0b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mcurrent_models_regions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_models_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mmodel_per_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_models_regions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mfarsightness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_per_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Piemonte'"
     ]
    }
   ],
   "source": [
    "to_check = name_current_save\n",
    "model_dir = name_current_save+\"_models\"#'results_v16_ma_poly_after_update_only_covid_models'\n",
    "\n",
    "\n",
    "path_datasets = \"{}predictions/{}\".format(saved, to_check)\n",
    "#path_train = \"{}_df_train\".format(path_datasets)\n",
    "path_test = \"{}_df_test.csv\".format(path_datasets)\n",
    "path_unseen = \"{}_df_unseen.csv\".format(path_datasets)\n",
    "\n",
    "df_test_selected = pd.read_csv(path_test)\n",
    "# TODO altro??\n",
    "df_unseen_selected = pd.read_csv(path_unseen)\n",
    "\n",
    "\n",
    "\n",
    "def init_models_dict(model_dir):\n",
    "    path_results_dir = join(saved, 'models', model_dir)\n",
    "    models = os.listdir(path_results_dir)\n",
    "    models_dict = {}\n",
    "    for model in models:\n",
    "        models_dict[model] = {}\n",
    "        model_path = join(path_results_dir, model)\n",
    "        regions = os.listdir(model_path)\n",
    "        for region in regions:\n",
    "            models_dict[model][region]={}\n",
    "            region_path = join(model_path, region)\n",
    "            model_names = sorted([f for f in os.listdir(region_path) if isfile(join(region_path, f))], key=lambda f: int(f.split(\"_\")[1].split(\".\")[0]))\n",
    "            for model_name in model_names:\n",
    "                n_lag = int(model_name.split(\"_\")[1].split(\".\")[0])\n",
    "                loaded_model = pickle.load(open(join(region_path, model_name), \"rb\" ))\n",
    "                models_dict[model][region][n_lag]=loaded_model\n",
    "    return models_dict\n",
    "\n",
    "# reusing same model for prediction\n",
    "# TODO usare dash per mostrare i 3 casi separati\n",
    "region='Piemonte'\n",
    "model_name = 'XGBoost'\n",
    "is_xgb, is_poly3, is_poly2, is_rf, is_lstm = model_name==\"XGBoost\", model_name==\"Poly3\", model_name==\"Poly2\", model_name==\"RandomForest\", model_name==\"EncDecLSTM\"\n",
    "\n",
    "current_models_regions = init_models_dict(model_dir)\n",
    "model_per_target = current_models_regions[model_name][region]\n",
    "\n",
    "farsightness = max(model_per_target.keys())\n",
    "\n",
    "num_samples_retrain = 120\n",
    "if is_xgb or is_rf:\n",
    "    num_samples_retrain = 120\n",
    "elif is_lstm:\n",
    "    num_samples_retrain = 80\n",
    "    \n",
    "last_date_seen = df_test_selected.index.get_level_values('Date').max()\n",
    "start_date_unseen = df_test_selected+datetime.timedelta(days=1)\n",
    "\n",
    "current_region_df = df_unseen_selected.xs(region, level='Regione')\n",
    "current_region_test_df = df_test_selected.xs(region, level='Regione')\n",
    "\n",
    "current_train, current_unseen = current_region_test_df, current_region_df\n",
    "\n",
    "# take only last num_samples_retrain\n",
    "current_train = current_train.iloc[-num_samples_retrain:]\n",
    "\n",
    "features_train = current_train[features_selected]\n",
    "target_series_train = {int(col.split(\"_\")[1]):current_train[col] for col in current_train.columns if \"target\" in col}\n",
    "features_unseen = current_unseen[features_selected]\n",
    "#target_series_unseen = {int(col.split(\"_\")[0]):current_unseen[col] for col in current_unseen.columns if \"target\" in col}\n",
    "\n",
    "num_features_t = len([col for col in features_selected if not any(char.isdigit() for char in col)])\n",
    "n_timesteps = features_train.shape[1]//num_features_t\n",
    "\n",
    "\n",
    "dates = features_unseen.index\n",
    "num_dates= len(dates)\n",
    "predictions, index_predictions = [], []\n",
    "idx_dates = list(range(0, num_dates, farsightness))\n",
    "dates_to_return = dates[idx_dates]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "steps_ahead = [f for f in model_per_target.keys() if f <= farsightness]\n",
    "\n",
    "n_timesteps = 8\n",
    "num_features_t = len(trafficKPIs)+len(covidKPIs)\n",
    "\n",
    "min_y, max_y = float(\"+inf\"), float(\"-inf\")\n",
    "\n",
    "for idx, idx_d in enumerate(idx_dates):\n",
    "    d = dates[idx_d]\n",
    "    feature_current = features_unseen.loc[d:d]#.to_numpy()\n",
    "    #print(feature_current)\n",
    "    if 'LSTM' in model_name:\n",
    "        feature_current = feature_current.to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "    #print(feature_current.index)\n",
    "    for step_ahead in steps_ahead:\n",
    "        current_model = model_per_target[step_ahead]\n",
    "        #print(feature_current.shape)\n",
    "        current_prediction = current_model.predict(feature_current.values)\n",
    "        #print(step_ahead)\n",
    "        if 'LSTM' in model_name:\n",
    "            try:\n",
    "                current_prediction = current_prediction[-1][-1][-1]\n",
    "            except:\n",
    "                current_prediction = current_prediction[-1][-1]\n",
    "            predictions.append(current_prediction)\n",
    "        else:\n",
    "            current_prediction = current_prediction[-1]\n",
    "            predictions.append(current_prediction)\n",
    "        #print(\"{}: {}\".format(step_ahead, current_prediction))\n",
    "        index_predictions.append(d + datetime.timedelta(days=step_ahead))\n",
    "    # TODO retrain!!! Simulate that\n",
    "    if idx < len(idx_dates)-1:\n",
    "        print(\"RETRAIN!!!\")\n",
    "        # I can retrain\n",
    "        next_d = d+datetime.timedelta(days=steps_ahead[-1])\n",
    "        current_train_df = current_region_df.loc[:next_d]\n",
    "        \n",
    "        current_train_df = current_train_df.iloc[-num_samples_retrain:]\n",
    "        current_features_train = current_train_df[features_selected]\n",
    "        \n",
    "        for key in model_per_target.keys():\n",
    "            model = model_per_target[key]\n",
    "            features_at_day_d = current_region_df.loc[:d]\n",
    "            target_series_train = {int(col.split(\"_\")[1]):current_train_df[col] for col in current_train_df.columns if \"target\" in col}\n",
    "            current_features_train, target_series_train[key] = current_features_train.dropna(), target_series_train[key].dropna()\n",
    "            common_idxs = current_features_train.index.intersection(target_series_train[key].index)\n",
    "            current_features_train = current_features_train.loc[common_idxs]\n",
    "            target_series_train[key] = target_series_train[key].loc[common_idxs]\n",
    "            if is_lstm:\n",
    "                \n",
    "                size_int = int(current_features_train.shape[0]*0.7)\n",
    "                current_features_train_1, current_features_train_2 = current_features_train.iloc[:size_int], current_features_train.iloc[size_int:]\n",
    "                current_target_train, current_target_val = target_series_train[key].loc[current_features_train_1.index], target_series_train[key].loc[current_features_train_2.index]\n",
    "                lstm_input_train = current_features_train_1.to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "                lstm_input_val = current_features_train_2.to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "\n",
    "                history = model.fit(x=lstm_input_train, y=current_target_train, epochs=MAX_EPOCHS, verbose=0,\n",
    "                                  validation_data=(lstm_input_val, current_target_val),\n",
    "                                  callbacks=[early_stopping])\n",
    "                \n",
    "                #callback = EarlyStopping(monitor='loss', patience=3)\n",
    "                #model.fit(lstm_input, target_series_train[key], epochs=100, batch_size=32, verbose=0, callbacks=[callback])\n",
    "            else:\n",
    "                model.fit(current_features_train.values, target_series_train[key])\n",
    "\n",
    "series_predictions = pd.Series(data=predictions, index=index_predictions)\n",
    "series_target = df_unseen_covid.xs(region, level='Regione').loc[start_date_unseen:]['R_mean']\n",
    "\n",
    "\n",
    "min_y = min(series_predictions.min(), series_target.min())\n",
    "max_y = max(series_predictions.max(), series_target.max())\n",
    "\n",
    "for d in [dates[i] for i in idx_dates]:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        line = dict(color=\"black\", width=2, dash='dash'),\n",
    "        marker_size=1,\n",
    "        x=[d, d],\n",
    "        y=[min_y, max_y],\n",
    "        showlegend=False\n",
    "    ))       \n",
    "\n",
    "#series_target.plot()\n",
    "#series_predictions.plot()\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "        line = dict(color=\"orange\", width=1),\n",
    "        marker_size=1,\n",
    "        x=series_predictions.index,\n",
    "        y=series_predictions,\n",
    "        showlegend=False\n",
    "    ))  \n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "        line = dict(color=\"red\", width=1),\n",
    "        marker_size=1,\n",
    "        x=series_target.index,\n",
    "        y=series_target,\n",
    "        showlegend=False\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qui si visualizza il corrente\n",
    "region='Lombardia'\n",
    "model_name = 'Poly2'\n",
    "is_xgb, is_poly3, is_poly2, is_rf, is_lstm = model_name==\"XGBoost\", model_name==\"Poly3\", model_name==\"Poly2\", model_name==\"RandomForest\", model_name==\"EncDecLSTM\"\n",
    "\n",
    "num_samples_retrain = 45\n",
    "if is_xgb or is_rf:\n",
    "    num_samples_retrain = 60\n",
    "elif is_lstm:\n",
    "    num_samples_retrain = 80\n",
    "\n",
    "last_date_seen = pd.to_datetime('2021-01-15')\n",
    "start_date_unseen = last_date_seen+datetime.timedelta(days=1)\n",
    "\n",
    "current_region_df = df_test_prediction.xs(region, level='Regione')\n",
    "\n",
    "current_train, current_unseen = current_region_df.loc[:last_date_seen], current_region_df.loc[start_date_unseen:]\n",
    "\n",
    "# take only last num_samples_retrain\n",
    "current_train = current_train.iloc[-num_samples_retrain:]\n",
    "\n",
    "features_train = current_train[features]\n",
    "target_series_train = {int(col.split(\"_\")[1]):current_train[col] for col in current_train.columns if \"target\" in col}\n",
    "features_unseen = current_unseen[features]\n",
    "#target_series_unseen = {int(col.split(\"_\")[0]):current_unseen[col] for col in current_unseen.columns if \"target\" in col}\n",
    "\n",
    "num_features_t = len(trafficKPIs)\n",
    "n_timesteps = features_train.shape[1]//num_features_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_poly2:\n",
    "    model_per_target = {key: make_pipeline(PolynomialFeatures(2), Ridge()) for key in target_series_train.keys()}\n",
    "elif is_poly3:\n",
    "    model_per_target = {key: make_pipeline(PolynomialFeatures(3), Ridge()) for key in target_series_train.keys()}\n",
    "elif is_rf:\n",
    "    model_per_target = {key: RandomForestRegressor() for key in target_series_train.keys()}\n",
    "elif is_xgb:\n",
    "    model_per_target = {key: XGBRegressor() for key in target_series_train.keys()}\n",
    "elif is_lstm:\n",
    "    model_per_target = {key: build_lstm_3(n_timesteps, num_features_t) for key in target_series_train.keys()}\n",
    "for key in model_per_target.keys():\n",
    "    model = model_per_target[key]\n",
    "    current_features_train = features_train.copy()\n",
    "    current_features_train, target_series_train[key] = current_features_train.dropna(), target_series_train[key].dropna()\n",
    "    current_features_train = current_features_train.loc[target_series_train[key].index]\n",
    "    target_series_train[key] = target_series_train[key].loc[current_features_train.index]\n",
    "    if is_lstm:\n",
    "        size_int = int(current_features_train.shape[0]*0.7)\n",
    "        current_features_train_1, current_features_train_2 = current_features_train.iloc[:size_int], current_features_train.iloc[size_int:]\n",
    "        current_target_train, current_target_val = target_series_train[key].loc[current_features_train_1.index], target_series_train[key].loc[current_features_train_2.index]\n",
    "        lstm_input_train = current_features_train_1.to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "        lstm_input_val = current_features_train_2.to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True)\n",
    "\n",
    "        model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                    optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "                    metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "        \n",
    "        MAX_EPOCHS = 128\n",
    "        \n",
    "        history = model.fit(x=lstm_input_train, y=current_target_train, epochs=MAX_EPOCHS,\n",
    "                          validation_data=(lstm_input_val, current_target_val),\n",
    "                          callbacks=[early_stopping])\n",
    "        \n",
    "    else:\n",
    "        model.fit(current_features_train, target_series_train[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "farsightness = max(model_per_target.keys())\n",
    "\n",
    "dates = features_unseen.index\n",
    "num_dates= len(dates)\n",
    "predictions, index_predictions = [], []\n",
    "idx_dates = list(range(0, num_dates, farsightness))\n",
    "dates_to_return = dates[idx_dates]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "steps_ahead = [f for f in model_per_target.keys() if f <= farsightness]\n",
    "\n",
    "n_timesteps = 8\n",
    "num_features_t = len(trafficKPIs)\n",
    "\n",
    "min_y, max_y = float(\"+inf\"), float(\"-inf\")\n",
    "\n",
    "use_already_trained = True\n",
    "if use_already_trained:\n",
    "    model_per_target = models_regions['Poly1']['Lombardia']\n",
    "\n",
    "for idx, idx_d in enumerate(idx_dates):\n",
    "    d = dates[idx_d]\n",
    "    feature_current = features_unseen.loc[d:d]#.to_numpy()\n",
    "    #print(feature_current)\n",
    "    if 'LSTM' in model_name:\n",
    "        feature_current = feature_current.to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "    #print(feature_current.index)\n",
    "    for step_ahead in steps_ahead:\n",
    "        current_model = model_per_target[step_ahead]\n",
    "        #print(feature_current.shape)\n",
    "        current_prediction = current_model.predict(feature_current)\n",
    "        #print(step_ahead)\n",
    "        if 'LSTM' in model_name:\n",
    "            try:\n",
    "                current_prediction = current_prediction[-1][-1][-1]\n",
    "            except:\n",
    "                current_prediction = current_prediction[-1][-1]\n",
    "            predictions.append(current_prediction)\n",
    "        else:\n",
    "            current_prediction = current_prediction[-1]\n",
    "            predictions.append(current_prediction)\n",
    "        #print(\"{}: {}\".format(step_ahead, current_prediction))\n",
    "        index_predictions.append(d + datetime.timedelta(days=step_ahead))\n",
    "    # TODO retrain!!! Simulate that\n",
    "    if idx < len(idx_dates)-1:\n",
    "        print(\"RETRAIN!!!\")\n",
    "        # I can retrain\n",
    "        next_d = d+datetime.timedelta(days=steps_ahead[-1])\n",
    "        current_train_df = current_region_df.loc[:next_d]\n",
    "        \n",
    "        current_train_df = current_train_df.iloc[-num_samples_retrain:]\n",
    "        current_features_train = current_train_df[features]\n",
    "        \n",
    "        for key in model_per_target.keys():\n",
    "            model = model_per_target[key]\n",
    "            features_at_day_d = current_region_df.loc[:d]\n",
    "            target_series_train = {int(col.split(\"_\")[1]):current_train_df[col] for col in current_train_df.columns if \"target\" in col}\n",
    "            current_features_train, target_series_train[key] = current_features_train.dropna(), target_series_train[key].dropna()\n",
    "            current_features_train = current_features_train.loc[target_series_train[key].index]\n",
    "            target_series_train[key] = target_series_train[key].loc[current_features_train.index]\n",
    "            if is_lstm:\n",
    "                \n",
    "                size_int = int(current_features_train.shape[0]*0.7)\n",
    "                current_features_train_1, current_features_train_2 = current_features_train.iloc[:size_int], current_features_train.iloc[size_int:]\n",
    "                current_target_train, current_target_val = target_series_train[key].loc[current_features_train_1.index], target_series_train[key].loc[current_features_train_2.index]\n",
    "                lstm_input_train = current_features_train_1.to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "                lstm_input_val = current_features_train_2.to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "\n",
    "                history = model.fit(x=lstm_input_train, y=current_target_train, epochs=MAX_EPOCHS, verbose=0,\n",
    "                                  validation_data=(lstm_input_val, current_target_val),\n",
    "                                  callbacks=[early_stopping])\n",
    "                \n",
    "                #callback = EarlyStopping(monitor='loss', patience=3)\n",
    "                #model.fit(lstm_input, target_series_train[key], epochs=100, batch_size=32, verbose=0, callbacks=[callback])\n",
    "            else:\n",
    "                model.fit(features_train_df, target_series_train[key])     \n",
    "\n",
    "series_predictions = pd.Series(data=predictions, index=index_predictions)\n",
    "series_target = df_covid_predictions.xs(region, level='Regione').loc[start_date_unseen:]['R_mean']\n",
    "\n",
    "\n",
    "min_y = min(series_predictions.min(), series_target.min())\n",
    "max_y = max(series_predictions.max(), series_target.max())\n",
    "\n",
    "for d in [dates[i] for i in idx_dates]:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        line = dict(color=\"black\", width=2, dash='dash'),\n",
    "        marker_size=1,\n",
    "        x=[d, d],\n",
    "        y=[min_y, max_y],\n",
    "        showlegend=False\n",
    "    ))       \n",
    "\n",
    "#series_target.plot()\n",
    "#series_predictions.plot()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "        line = dict(color=\"orange\", width=1),\n",
    "        marker_size=1,\n",
    "        x=series_predictions.index,\n",
    "        y=series_predictions,\n",
    "        showlegend=False\n",
    "    ))  \n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "        line = dict(color=\"red\", width=1),\n",
    "        marker_size=1,\n",
    "        x=series_target.index,\n",
    "        y=series_target,\n",
    "        showlegend=False\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic-covid",
   "language": "python",
   "name": "traffic-covid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
