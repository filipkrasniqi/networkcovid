{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import gamma, poisson\n",
    "\n",
    "import epyestim\n",
    "import epyestim.covid19 as covid19\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import datetime\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE, r2_score\n",
    "from xgboost import XGBRegressor, DMatrix, train\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, TimeDistributed, RepeatVector\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras\n",
    "\n",
    "import plotly.express as px\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "\n",
    "to_sum_KPIs = ['totale_casi_giornalieri', 'terapia_intensiva_giornalieri', 'terapia_intensiva', 'nuovi_positivi', 'tamponi_giornalieri']\n",
    "covidKPIsPrecompute = ['%pos']+to_sum_KPIs\n",
    "trafficKPIsPrecompute = ['Handover', 'Download vol.', 'Upload vol.', '#Users']\n",
    "\n",
    "data_path = \"/Users/filipkrasniqi/Documents/Datasets.tmp/traffic-covid/\"\n",
    "saved = \"{}saved/\".format(data_path)\n",
    "traffic_daily = \"{}TS_1800_daily.pkl\".format(saved)\n",
    "region_traffic_daily = \"{}all.pkl\".format(saved)\n",
    "covid = \"{}covid/\".format(data_path)\n",
    "covid_daily = \"{}covid_2303.csv\".format(covid)\n",
    "\n",
    "capped_last_date = pd.to_datetime('2021-01-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "path_covid_predictions=\"{}predictions/covid_2303.pkl\".format(saved)\n",
    "recompute_rt = False\n",
    "if recompute_rt:\n",
    "    for r in regions:\n",
    "        print(\"REGIONE: {}\".format(r))\n",
    "        current_df = df_covid.loc[df_covid['Regione'] == r]\n",
    "        current_df['Date'] = pd.to_datetime(current_df['Date']).dt.date\n",
    "        current_df['DateIndex'] = current_df.loc[:, 'Date']\n",
    "        current_df.set_index('DateIndex', inplace=True)\n",
    "        #current_df = current_df.loc[current_df['nuovi_positivi'] > 0]\n",
    "        current_df = current_df.loc[pd.to_datetime('2020/03/01'):]\n",
    "        idxs = (current_df['nuovi_positivi'] < 0)# | (current_df.isna()) | (current_df['nuovi_positivi'] == np.inf) | (current_df['nuovi_positivi'] == -np.inf)\n",
    "        if idxs.sum() > 0:\n",
    "            current_df.loc[idxs, 'nuovi_positivi'] = np.nan\n",
    "        current_df.fillna(method='ffill', inplace=True)\n",
    "        current_df.dropna(subset=['nuovi_positivi'], inplace=True)\n",
    "        #current_df[current_df.loc[:, 'nuovi_positivi']]\n",
    "        #current_df.dropna(subset=['nuovi_positivi'], inplace=True)\n",
    "        #\n",
    "        current_df = current_df.drop_duplicates(keep='first')\n",
    "        #print(current_df['nuovi_positivi'].shape, current_df['nuovi_positivi'].apply(lambda x: x < 0).sum())\n",
    "        #current_df.dropna(subset=['totale_casi_giornalieri'], inplace=True)\n",
    "        #print(current_df['totale_casi_giornalieri'].isna().sum())\n",
    "        #print(current_df['totale_casi_giornalieri'].sum())\n",
    "        r_t_series = covid19.r_covid(current_df['nuovi_positivi'])\n",
    "        current_df = pd.merge(current_df, r_t_series, left_index=True, right_index=True)\n",
    "        dfs.append(current_df)\n",
    "    df_covid_predictions = pd.concat(dfs)\n",
    "    del dfs\n",
    "    df_covid_predictions.set_index(['Date', 'Regione'], inplace=True)\n",
    "    df_covid_predictions['%pos'] = (df_covid_predictions['nuovi_positivi']/df_covid_predictions['tamponi_giornalieri'])\n",
    "    df_covid_predictions.to_pickle(path_covid_predictions)\n",
    "else:\n",
    "    df_covid_predictions = pd.read_pickle(path_covid_predictions)\n",
    "\n",
    "df_unseen_covid = df_covid_predictions.loc[df_covid_predictions.index.get_level_values('Date')>=capped_last_date]\n",
    "#df_covid_predictions = df_covid_predictions.loc[df_covid_predictions.index.get_level_values('Date')<capped_last_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO importare modelli\n",
    "def init_models_dict(model_dir):\n",
    "    path_results_dir = join(saved, 'models', model_dir)\n",
    "    models = os.listdir(path_results_dir)\n",
    "    models_dict = {}\n",
    "    #model_path = join(path_results_dir, model)\n",
    "    regions = os.listdir(path_results_dir)\n",
    "    for region in regions:\n",
    "        region_path = join(path_results_dir, region)\n",
    "        model = keras.models.load_model(region_path)\n",
    "        models_dict[region]=model\n",
    "        #model_names = sorted([f for f in os.listdir(region_path) if isfile(join(region_path, f))], key=lambda f: int(f.split(\"_\")[1].split(\".\")[0]))\n",
    "    return models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_datasets(to_check):\n",
    "    model_dir = to_check+\"_models\"#'results_v16_ma_poly_after_update_only_covid_models'\n",
    "\n",
    "    path_datasets = \"{}predictions/{}\".format(saved, to_check)\n",
    "    #path_train = \"{}_df_train\".format(path_datasets)\n",
    "    path_test = \"{}_df_test.csv\".format(path_datasets)\n",
    "    path_unseen = \"{}_df_unseen.csv\".format(path_datasets)\n",
    "\n",
    "    df_test_selected = pd.read_csv(path_test)\n",
    "    df_test_selected['Date'] = pd.to_datetime(df_test_selected['Date'])\n",
    "    df_test_selected.set_index(['Date', 'Regione'], inplace=True)\n",
    "    \n",
    "    df_unseen_selected = pd.read_csv(path_unseen)\n",
    "    df_unseen_selected['Date'] = pd.to_datetime(df_unseen_selected['Date'])\n",
    "    df_unseen_selected.set_index(['Date', 'Regione'], inplace=True)\n",
    "    \n",
    "    current_models_regions = init_models_dict(model_dir)\n",
    "    return df_test_selected, df_unseen_selected, current_models_regions\n",
    "\n",
    "def figure_from_name_region_model(fig, to_check, region, farsightness, row):\n",
    "    df_test_selected, df_unseen_selected, current_models_regions = init_datasets(to_check)\n",
    "\n",
    "    current_model = current_models_regions[region]\n",
    "\n",
    "    num_samples_retrain = 64\n",
    "\n",
    "    last_date_seen = df_test_selected.index.get_level_values('Date').max()\n",
    "    start_date_unseen = last_date_seen+datetime.timedelta(days=1)\n",
    "\n",
    "    current_region_df = df_unseen_selected.xs(region, level='Regione')\n",
    "    current_region_test_df = df_test_selected.xs(region, level='Regione')\n",
    "\n",
    "    current_train, current_unseen = current_region_test_df, current_region_df\n",
    "\n",
    "    # take only last num_samples_retrain\n",
    "    current_train = current_train.iloc[-num_samples_retrain:]\n",
    "    features_selected = [col for col in current_train.columns if \"target\" not in col]\n",
    "\n",
    "    features_train = current_train[features_selected]\n",
    "    features_unseen = current_unseen[features_selected]\n",
    "\n",
    "    num_features_t = len([col for col in features_selected if not any(char.isdigit() for char in col)])\n",
    "    n_timesteps = features_train.shape[1]//num_features_t\n",
    "\n",
    "    dates = features_unseen.index\n",
    "    num_dates= len(dates)\n",
    "    predictions, index_predictions = [], []\n",
    "    idx_dates = list(range(0, num_dates, farsightness))\n",
    "    dates_to_return = dates[idx_dates]\n",
    "\n",
    "    min_y, max_y = float(\"+inf\"), float(\"-inf\")\n",
    "\n",
    "    for idx, idx_d in enumerate(idx_dates):\n",
    "        d = dates[idx_d]\n",
    "        print(\"TEST {}\".format(d))\n",
    "        feature_current = features_unseen.loc[d:d]#.to_numpy()\n",
    "        #print(feature_current)\n",
    "        feature_current = feature_current.to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "        #print(feature_current.shape)\n",
    "        current_predictions = current_model.predict(feature_current).flatten()[:farsightness]\n",
    "        print(\"DBG: {}\".format(current_predictions.shape))\n",
    "        for p in current_predictions:\n",
    "            predictions.append(p)\n",
    "        next_d = d + datetime.timedelta(days=farsightness)\n",
    "        for d in [d for d in pd.date_range(d, next_d)]:\n",
    "            index_predictions.append(d)\n",
    "        \n",
    "        if idx < len(idx_dates)-1:\n",
    "            print(\"RETRAIN\")\n",
    "            # I can retrain\n",
    "            current_train_df = current_region_df.loc[:next_d]\n",
    "\n",
    "            current_train_df = current_train_df.iloc[-num_samples_retrain:]\n",
    "            current_features_train = current_train_df[features_selected]\n",
    "\n",
    "            #for key in model_per_target.keys():\n",
    "            #model = model_per_target[key]\n",
    "            features_at_day_d = current_region_df.loc[:d]\n",
    "            target_series_train = current_train_df[[col for col in current_train_df.columns if \"target\" in col]]\n",
    "            current_features_train, target_series_train = current_features_train.dropna(), target_series_train.dropna()\n",
    "            common_idxs = current_features_train.index.intersection(target_series_train.index)\n",
    "            current_features_train = current_features_train.loc[common_idxs]\n",
    "            target_series_train = target_series_train.loc[common_idxs]\n",
    "\n",
    "            size_int = int(current_features_train.shape[0]*0.7)\n",
    "            current_features_train_1, current_features_train_2 = current_features_train.iloc[:size_int], current_features_train.iloc[size_int:]\n",
    "            current_target_train, current_target_val = target_series_train.loc[current_features_train_1.index], target_series_train.loc[current_features_train_2.index]\n",
    "            lstm_input_train = current_features_train_1.to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "            lstm_input_val = current_features_train_2.to_numpy().reshape(-1, n_timesteps, num_features_t, order='C')\n",
    "            \n",
    "            MAX_EPOCHS = 128\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True)\n",
    "            \n",
    "            if lstm_input_train.shape[0] > 0:\n",
    "                history = current_model.fit(x=lstm_input_train, y=current_target_train, epochs=MAX_EPOCHS, verbose=0,\n",
    "                                  validation_data=(lstm_input_val, current_target_val),\n",
    "                                  callbacks=[early_stopping])\n",
    "    \n",
    "    predictions = np.array(predictions).flatten()\n",
    "    print(\"Predictions ({}) -> {}\".format(len(predictions), predictions))\n",
    "    print(\"Indices ({}) -> {}\".format(len(index_predictions), index_predictions))\n",
    "    \n",
    "    index_predictions = index_predictions[:len(predictions)]\n",
    "    series_predictions = pd.Series(data=predictions, index=index_predictions)\n",
    "    series_target = df_unseen_covid.xs(region, level='Regione').loc[start_date_unseen:]['R_mean']\n",
    "\n",
    "\n",
    "    min_y = min(series_predictions.min(), series_target.min())\n",
    "    max_y = max(series_predictions.max(), series_target.max())\n",
    "\n",
    "    for d in [dates[i] for i in idx_dates]:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            line = dict(color=\"black\", width=2, dash='dash'),\n",
    "            marker_size=1,\n",
    "            x=[d, d],\n",
    "            y=[min_y, max_y],\n",
    "            showlegend=False\n",
    "        ), row=row, col=1)       \n",
    "\n",
    "    #series_target.plot()\n",
    "    #series_predictions.plot()\n",
    "\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "            line = dict(color=\"orange\", width=1),\n",
    "            marker_size=1,\n",
    "            x=series_predictions.index,\n",
    "            y=series_predictions,\n",
    "            showlegend=False\n",
    "        ), row=row, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "            line = dict(color=\"red\", width=1),\n",
    "            marker_size=1,\n",
    "            x=series_target.index,\n",
    "            y=series_target,\n",
    "            showlegend=False\n",
    "        ), row=row, col=1)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_models = False\n",
    "if cache_models:\n",
    "    to_check = 'results_v5_LSTM'\n",
    "    to_check_both, to_check_traffic, to_check_rmean = \"{}_both\".format(to_check), \"{}_only_traffic\".format(to_check), \"{}_only_rmean\".format(to_check)\n",
    "    to_check_traffic_no_mobility = \"{}_only_traffic_no_mobility\".format(to_check)\n",
    "\n",
    "    df_test_selected, df_unseen_selected, _ = init_datasets(to_check_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:49008/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x16f6d48b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 2021-01-16 00:00:00\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x170776e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "DBG: (9,)\n",
      "RETRAIN\n",
      "TEST 2021-01-25 00:00:00\n",
      "DBG: (9,)\n",
      "RETRAIN\n",
      "TEST 2021-02-03 00:00:00\n",
      "DBG: (9,)\n",
      "RETRAIN\n",
      "TEST 2021-02-12 00:00:00\n",
      "DBG: (9,)\n",
      "Predictions (36) -> [0.85292673 0.8591075  0.86302966 0.8705704  0.8940509  0.926978\n",
      " 0.95881134 0.9849428  1.0033805  1.0043985  1.0269228  0.99807334\n",
      " 0.97245604 0.94707704 0.91690713 0.9025933  0.8930854  0.88669324\n",
      " 1.0184944  1.0486392  1.0396134  1.0188608  1.0056682  0.9884053\n",
      " 0.96393734 0.9458586  0.942184   1.0479251  1.051803   1.0391144\n",
      " 1.0166308  1.005773   0.9957253  0.98209417 0.96584016 0.96366036]\n",
      "Indices (40) -> [Timestamp('2021-01-16 00:00:00', freq='D'), Timestamp('2021-01-17 00:00:00', freq='D'), Timestamp('2021-01-18 00:00:00', freq='D'), Timestamp('2021-01-19 00:00:00', freq='D'), Timestamp('2021-01-20 00:00:00', freq='D'), Timestamp('2021-01-21 00:00:00', freq='D'), Timestamp('2021-01-22 00:00:00', freq='D'), Timestamp('2021-01-23 00:00:00', freq='D'), Timestamp('2021-01-24 00:00:00', freq='D'), Timestamp('2021-01-25 00:00:00', freq='D'), Timestamp('2021-01-25 00:00:00', freq='D'), Timestamp('2021-01-26 00:00:00', freq='D'), Timestamp('2021-01-27 00:00:00', freq='D'), Timestamp('2021-01-28 00:00:00', freq='D'), Timestamp('2021-01-29 00:00:00', freq='D'), Timestamp('2021-01-30 00:00:00', freq='D'), Timestamp('2021-01-31 00:00:00', freq='D'), Timestamp('2021-02-01 00:00:00', freq='D'), Timestamp('2021-02-02 00:00:00', freq='D'), Timestamp('2021-02-03 00:00:00', freq='D'), Timestamp('2021-02-03 00:00:00', freq='D'), Timestamp('2021-02-04 00:00:00', freq='D'), Timestamp('2021-02-05 00:00:00', freq='D'), Timestamp('2021-02-06 00:00:00', freq='D'), Timestamp('2021-02-07 00:00:00', freq='D'), Timestamp('2021-02-08 00:00:00', freq='D'), Timestamp('2021-02-09 00:00:00', freq='D'), Timestamp('2021-02-10 00:00:00', freq='D'), Timestamp('2021-02-11 00:00:00', freq='D'), Timestamp('2021-02-12 00:00:00', freq='D'), Timestamp('2021-02-12 00:00:00', freq='D'), Timestamp('2021-02-13 00:00:00', freq='D'), Timestamp('2021-02-14 00:00:00', freq='D'), Timestamp('2021-02-15 00:00:00', freq='D'), Timestamp('2021-02-16 00:00:00', freq='D'), Timestamp('2021-02-17 00:00:00', freq='D'), Timestamp('2021-02-18 00:00:00', freq='D'), Timestamp('2021-02-19 00:00:00', freq='D'), Timestamp('2021-02-20 00:00:00', freq='D'), Timestamp('2021-02-21 00:00:00', freq='D')]\n",
      "TEST 2021-01-16 00:00:00\n",
      "DBG: (34,)\n",
      "RETRAIN\n",
      "TEST 2021-02-19 00:00:00\n",
      "DBG: (34,)\n",
      "Predictions (68) -> [0.85292673 0.8591075  0.86302966 0.8705704  0.8940509  0.926978\n",
      " 0.95881134 0.9849428  1.0033805  1.012301   1.0132865  1.0080338\n",
      " 1.0159019  1.0183905  1.01555    1.0076683  0.99738634 0.98740613\n",
      " 0.98022854 0.97813517 0.980502   0.9830175  0.9932035  1.0110859\n",
      " 1.0332558  1.0552667  1.0744891  1.0896815  1.101137   1.107816\n",
      " 1.1102296  1.1089692  1.1047294  1.098083   1.0746212  1.0703535\n",
      " 1.0614345  1.0433273  1.0347744  1.0326667  1.024221   1.007502\n",
      " 0.9920213  0.9861332  0.9758634  0.95999414 0.9434013  0.93030417\n",
      " 0.92341095 0.92414796 0.93264514 0.9492334  0.9723327  0.9985678\n",
      " 1.0237552  1.0455816  1.062275   1.0733099  1.0801014  1.0831053\n",
      " 1.08314    1.0811276  1.077944   1.0743635  1.0709934  1.0682501\n",
      " 1.0663646  1.0654101 ]\n",
      "Indices (70) -> [Timestamp('2021-01-16 00:00:00', freq='D'), Timestamp('2021-01-17 00:00:00', freq='D'), Timestamp('2021-01-18 00:00:00', freq='D'), Timestamp('2021-01-19 00:00:00', freq='D'), Timestamp('2021-01-20 00:00:00', freq='D'), Timestamp('2021-01-21 00:00:00', freq='D'), Timestamp('2021-01-22 00:00:00', freq='D'), Timestamp('2021-01-23 00:00:00', freq='D'), Timestamp('2021-01-24 00:00:00', freq='D'), Timestamp('2021-01-25 00:00:00', freq='D'), Timestamp('2021-01-26 00:00:00', freq='D'), Timestamp('2021-01-27 00:00:00', freq='D'), Timestamp('2021-01-28 00:00:00', freq='D'), Timestamp('2021-01-29 00:00:00', freq='D'), Timestamp('2021-01-30 00:00:00', freq='D'), Timestamp('2021-01-31 00:00:00', freq='D'), Timestamp('2021-02-01 00:00:00', freq='D'), Timestamp('2021-02-02 00:00:00', freq='D'), Timestamp('2021-02-03 00:00:00', freq='D'), Timestamp('2021-02-04 00:00:00', freq='D'), Timestamp('2021-02-05 00:00:00', freq='D'), Timestamp('2021-02-06 00:00:00', freq='D'), Timestamp('2021-02-07 00:00:00', freq='D'), Timestamp('2021-02-08 00:00:00', freq='D'), Timestamp('2021-02-09 00:00:00', freq='D'), Timestamp('2021-02-10 00:00:00', freq='D'), Timestamp('2021-02-11 00:00:00', freq='D'), Timestamp('2021-02-12 00:00:00', freq='D'), Timestamp('2021-02-13 00:00:00', freq='D'), Timestamp('2021-02-14 00:00:00', freq='D'), Timestamp('2021-02-15 00:00:00', freq='D'), Timestamp('2021-02-16 00:00:00', freq='D'), Timestamp('2021-02-17 00:00:00', freq='D'), Timestamp('2021-02-18 00:00:00', freq='D'), Timestamp('2021-02-19 00:00:00', freq='D'), Timestamp('2021-02-19 00:00:00', freq='D'), Timestamp('2021-02-20 00:00:00', freq='D'), Timestamp('2021-02-21 00:00:00', freq='D'), Timestamp('2021-02-22 00:00:00', freq='D'), Timestamp('2021-02-23 00:00:00', freq='D'), Timestamp('2021-02-24 00:00:00', freq='D'), Timestamp('2021-02-25 00:00:00', freq='D'), Timestamp('2021-02-26 00:00:00', freq='D'), Timestamp('2021-02-27 00:00:00', freq='D'), Timestamp('2021-02-28 00:00:00', freq='D'), Timestamp('2021-03-01 00:00:00', freq='D'), Timestamp('2021-03-02 00:00:00', freq='D'), Timestamp('2021-03-03 00:00:00', freq='D'), Timestamp('2021-03-04 00:00:00', freq='D'), Timestamp('2021-03-05 00:00:00', freq='D'), Timestamp('2021-03-06 00:00:00', freq='D'), Timestamp('2021-03-07 00:00:00', freq='D'), Timestamp('2021-03-08 00:00:00', freq='D'), Timestamp('2021-03-09 00:00:00', freq='D'), Timestamp('2021-03-10 00:00:00', freq='D'), Timestamp('2021-03-11 00:00:00', freq='D'), Timestamp('2021-03-12 00:00:00', freq='D'), Timestamp('2021-03-13 00:00:00', freq='D'), Timestamp('2021-03-14 00:00:00', freq='D'), Timestamp('2021-03-15 00:00:00', freq='D'), Timestamp('2021-03-16 00:00:00', freq='D'), Timestamp('2021-03-17 00:00:00', freq='D'), Timestamp('2021-03-18 00:00:00', freq='D'), Timestamp('2021-03-19 00:00:00', freq='D'), Timestamp('2021-03-20 00:00:00', freq='D'), Timestamp('2021-03-21 00:00:00', freq='D'), Timestamp('2021-03-22 00:00:00', freq='D'), Timestamp('2021-03-23 00:00:00', freq='D'), Timestamp('2021-03-24 00:00:00', freq='D'), Timestamp('2021-03-25 00:00:00', freq='D')]\n"
     ]
    }
   ],
   "source": [
    "name = \"Compare input on unseen data\"\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "app_results = JupyterDash(name, external_stylesheets=external_stylesheets)\n",
    "\n",
    "all_regions = df_test_selected.index.get_level_values('Regione').unique()\n",
    "\n",
    "app_results.layout = html.Div([\n",
    "html.Label(\n",
    "    [\n",
    "        \"Regione\",\n",
    "        dcc.Dropdown(id=\"region\",\n",
    "                     options=[{\"label\": x, \"value\": x} for x in all_regions],\n",
    "                    value=\"Lombardia\",\n",
    "                    clearable=False)\n",
    "    ]),\n",
    "\n",
    "html.Label(\n",
    "    [\n",
    "        \"Farsightness\",\n",
    "        dcc.Dropdown(id=\"farsightness\",\n",
    "                     options=[{\"label\": x, \"value\": x} for x in [9, 17, 27, 34]],\n",
    "                    value=9,\n",
    "                    clearable=False)\n",
    "    ]),\n",
    "\n",
    "html.Label(\n",
    "            [\"Rolling average window \",\n",
    "            html.Br(),\n",
    "            dcc.Input(\n",
    "                id='rolling_avg',\n",
    "                type='number',\n",
    "                value=1\n",
    "            )]\n",
    "),\n",
    "html.Div(dcc.Graph(id=name))])\n",
    "\n",
    "@app_results.callback(\n",
    "Output(name, \"figure\"), \n",
    "[Input(\"region\", \"value\"), Input(\"farsightness\", \"value\"), Input(\"rolling_avg\", \"value\")])\n",
    "def display_rf_results(region, farsightness, roll_avg):\n",
    "    \n",
    "    fig = make_subplots(vertical_spacing = 0.01, horizontal_spacing = 0.01, rows=4, cols=1, \\\n",
    "                    shared_xaxes=True, shared_yaxes=True, row_titles=['All', 'T+M', 'Only T', 'Only C'], column_titles=['Unseen: target vs prediction'])#subplot_titles=subplot_titles)\n",
    "\n",
    "    fig = figure_from_name_region_model(fig, to_check_both, region, farsightness, 1)\n",
    "    #fig = figure_from_name_region_model(fig, to_check_traffic, region, model_name, farsightness, 2)\n",
    "    #fig = figure_from_name_region_model(fig, to_check_traffic_no_mobility, region, model_name, farsightness, 3)\n",
    "    #fig = figure_from_name_region_model(fig, to_check_rmean, region, model_name, farsightness, 4)\n",
    "    \n",
    "    \n",
    "    return fig\n",
    "\n",
    "#app_timeseries = build_app_timeseries(df_traffic_daily_SO, df_covid_SO)\n",
    "app_results.run_server(mode='inline', port=49008) # debug=True, use_reloader=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic-covid",
   "language": "python",
   "name": "traffic-covid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
